{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import numpy.random as nprand\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with k-Armed Bandits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will implement the algorithms presented in chapter 2 of Sutton's book and try to reproduce its figures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bandits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following classes can be used to generate bandits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Bandit():\n",
    "    # a one armed bandit that gives normally distributed rewards\n",
    "    # with mean 'q_star' and variance 'scale'\n",
    "    def __init__(self, q_star, scale):\n",
    "        self.mean = q_star\n",
    "        self.scale = scale\n",
    "        \n",
    "    def reward(self):\n",
    "        return nprand.normal(self.mean, self.scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class kBandits():\n",
    "    # a collection of k 1-armed bandits, normally distributed with\n",
    "    #Â mean 'mean' and variance 'scale'\n",
    "    def __init__(self, k, mean, scale):\n",
    "        self.bandits = [Bandit(nprand.normal(mean, scale), 1) for i in range(0, k)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a 10-armed bandit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "K = kBandits(10, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we generate rewards to plot a figure like Fig 4.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we add a slight jitter in x to our points, for a clearer plot\n",
    "points = [[i+nprand.normal(0, 0.05), K.bandits[i].reward()] for i in range(0, 10) for k in range(0, 100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [p[0] for p in points]\n",
    "y = [p[1] for p in points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ax = plt.scatter(x, y)\n",
    "plt.xlabel('Action')\n",
    "plt.ylabel('Reward')\n",
    "plt.title('Reward distribution for a 10-armed bandit')\n",
    "plt.xlim([-1, 10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: implementing the first algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To implement the first algorithm of the chapter, you'll need an argmax function that breaks ties randomly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def randargmax(array):\n",
    "    # your function here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint: take a look at `numpy.where` and `numpy.random.choice`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll also need a function that decides wether the action should be to exploratory (random), with proba epsilon, or greedy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def choose_at_random(epsilon):\n",
    "    # your function here\n",
    "    # should return 1 if exploration, 0 otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You're ready to implement the first algorithm!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simple_algo(steps, epsilon):\n",
    "    # your algorithm here\n",
    "    # it should return an array of rewards, one per step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reproduce Fig 2.2, the algorithm should be run 2000 times with 1000 steps, and the mean should be taken over the 2000 rewards per step. Such operations can be easily managed using `numpy` in the following way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_episodes = 2000\n",
    "n_steps = 1000\n",
    "\n",
    "# initiate an empty array of the desired shape\n",
    "rewards = np.ndarray(shape=(n_episodes, n_steps))\n",
    "\n",
    "# fill the array with the results of the algorithm\n",
    "for i in range(0, np.size(rews, 0)):\n",
    "    R = simple_algo(n_steps, epsilon)\n",
    "    # replace the ith row with the computed rewards\n",
    "    rewards[i, :] = R\n",
    "    \n",
    "# then take the mean over the rows using array.mean\n",
    "mean_reward = rews.mean(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now plot the result and compare it to the book's figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(mean_r)\n",
    "plt.xlabel('Steps')\n",
    "plt.ylabel('Average reward')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower part of Fig. 2.2 shows the percentage of optimal actions chosen by the algorithm. Modify your `simple_algo` to keep track of optimal actions and try to reproduce the book's bottom figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
