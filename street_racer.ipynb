{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as nprand\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Street Racer\n",
    "\n",
    "In this notebook, you'll apply the methods of chapter 4 of Sutton's book to a simple racing problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem consists in driving a car as fast of possible over an exact distance $L$, and stopping there.\n",
    "\n",
    "This distance is divided in steps $0, ..., L$. The car can drive at three different speed: _low_, _medium_, _high_. Leaving step $j$ at _low_ speed, it will move to $j+1$. _Medium_ and _high_ bring it to $j+2$ and $j+3$, respectively.\n",
    "\n",
    "At any step, the driver can decide to _decelerate_, _maintain speed_ or _accelerate_. Decelarating will cause the car to leave its current place at one speed lower. If the car is already at _low_ speed, decelarating keeps it in the same spot. Maintaining speed does exactly what you think. Accelerating will increase the speed by one, except at _high_ speed, where it is equivalent to maintaining speed.\n",
    "\n",
    "The car starts on step $0$ at _low_ speed.\n",
    "\n",
    "Beyond the $L$ distance there is a huge, hot lake of lava. Needless to say, the car must be able to stop at $L$, or the driver will suffer quite a lot."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    car ->                    ||/\\/\\/\n",
    "     |_______________ ... ____||/\\/ lava\n",
    "     |    |    |    |       | ||/\\/\\/\n",
    "     0    1    2    3       L ||/\\/\\/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help the driver win the race and not die, build a model of the problem and apply the policy iteration and value iteration methods to find her optimal trajectory.\n",
    "\n",
    "As this problem is an (over-)simplification of our traffic light problem, any work done here could serve as a building block for later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by figuring out the number of states you will need and build transition matrices for every action. For now, actions move the car from state to state in the deterministic manner described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "T_decelerate = np.zeros(shape=(3*l+1, 3*l+1))\n",
    "T_maintain = np.zeros(shape=(3*l+1, 3*l+1))\n",
    "T_accel = np.zeros(shape=(3*l+1, 3*l+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have three states for each position then we have a lava state (absorbing state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the matrix repeats itself for each action :  (here the decelerate matrix)\n",
    "\n",
    "\\begin{pmatrix}1&0&0 & 0&0&0 & 0&0&0 &  0&0&0 & \\ldots \n",
    "\\\\ 0&0&0 & 1&0&0 & 0&0&0 &  0&0&0 & \\ldots \n",
    "\\\\ 0&0&0 & 0&0&0 & 0&1&0 &  0&0&0 & \\ldots\n",
    "\\\\ 0&0&0 & 1&0&0 & 0&0&0 &  0&0&0 & \\ldots\n",
    "\\\\ 0&0&0 & 0&0&0 & 1&0&0 &  0&0&0 & \\ldots\n",
    "\\\\ 0&0&0 & 0&0&0 & 0&0&0 &  0&1&0 & \\ldots\n",
    "\\end{pmatrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n=len(T_decelerate)\n",
    "\n",
    "for i in range(n-1):\n",
    "    if (i%3==0 ):\n",
    "        T_decelerate[i][i]=1        #if at low speed and decelerating : stay on the same spot at low speed\n",
    "    if (i%3==1):                \n",
    "        if (i+2>n-1):\n",
    "            T_decelerate[i][n-1]=1  #if action takes us beyond l : we fall into the lava-state  \n",
    "        else :     \n",
    "            T_decelerate[i][i+2]=1  #other wise we just end up in the next position with low speed since we are in medium \n",
    "    if (i%3==2):\n",
    "        if (i+5>n-1):\n",
    "            T_decelerate[i][n-1]=1  #if action takes us beyond l : we fall into the lava-state \n",
    "        else : \n",
    "            T_decelerate[i][i+5]=1  #otherwise we just end up two states after with medium speed \n",
    "                       \n",
    "\n",
    "for i in range(n-1):\n",
    "    if (i%3==0):\n",
    "        if (i+7>n-1):\n",
    "            T_accel[i][n-1]=1\n",
    "        else :\n",
    "            T_accel[i][i+7]=1\n",
    "        if (i+3 > n-1 ):\n",
    "            T_maintain[i][n-1]=1\n",
    "        else:\n",
    "            T_maintain[i][i+3]=1\n",
    "    if (i%3==1):\n",
    "        if (i+10>n-1):\n",
    "            T_accel[i][n-1]=1\n",
    "        else : \n",
    "            T_accel[i][i+10]=1\n",
    "        if (i+6>n-1):\n",
    "            T_maintain[i][n-1]=1\n",
    "        else :\n",
    "            T_maintain[i][i+6]=1\n",
    "    if (i%3==2):\n",
    "        if(i+9>n-1):\n",
    "            T_accel[i][n-1]=1\n",
    "            T_maintain[i][n-1]=1\n",
    "        else :\n",
    "            T_accel[i][i+9]=1\n",
    "            T_maintain[i][i+9]=1\n",
    "\n",
    "            \n",
    "#once dead you stay dead !            \n",
    "T_accel[n-1][n-1]=1\n",
    "T_maintain[n-1][n-1]=1\n",
    "T_decelerate[n-1][n-1]=1 \n",
    "\n",
    "\n",
    "#define L_low as an absorbing state\n",
    "T_accel[-4]=np.zeros(3*l+1)\n",
    "T_decelerate[-4]=np.zeros(3*l+1)\n",
    "T_maintain[-4]=np.zeros(3*l+1)\n",
    "\n",
    "T_accel[-4][-4]=1\n",
    "T_maintain[-4][-4]=1\n",
    "T_decelerate[-4][-4]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we check for any encoding issues in the matrices : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def error_matrix(A):\n",
    "    result = np.dot(A,np.ones(len(A)))\n",
    "    print(\"erreur de type pas d'actions associées \",np.where(result == 0)[0])\n",
    "    print(\"erreur de type deux actions ou plus associées \",np.where(result >= 2)[0],\":\")\n",
    "    for i in np.where(result >= 2)[0]:\n",
    "        print(\"sum of the line : \",result[i])\n",
    "        print(A[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deceleration matrix\n",
      "erreur de type pas d'actions associées  []\n",
      "erreur de type deux actions ou plus associées  [] :\n",
      "acceleration matrix\n",
      "erreur de type pas d'actions associées  []\n",
      "erreur de type deux actions ou plus associées  [] :\n",
      "maintain matrix\n",
      "erreur de type pas d'actions associées  []\n",
      "erreur de type deux actions ou plus associées  [] :\n"
     ]
    }
   ],
   "source": [
    "print(\"deceleration matrix\")\n",
    "error_matrix(T_decelerate)\n",
    "print(\"acceleration matrix\")\n",
    "error_matrix(T_accel)\n",
    "print(\"maintain matrix\")\n",
    "error_matrix(T_maintain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And define the reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "R = -np.ones(3*l+1)\n",
    "lava = -10^l*100\n",
    "R[-1]=  lava  #lava state : you die\n",
    "R[-4]=10^l         #L in low speed ! win ! \n",
    "\n",
    "#are these ones really usefull ? bad reward when entering L state with medium or high speed\n",
    "R[-2]= lava\n",
    "R[-3]= lava"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining a policy : at the beginning we choose a random policy. A policy is here encoded as a sequence of letters : 1,2,3 for decelerate, maintain, accelerate. The sequence is of length 3*l+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "policy_initial = np.zeros(3*l+1)\n",
    "p_rand1=np.zeros(3*l+1)\n",
    "p_rand2=np.zeros(3*l+1)\n",
    "for i in range(len(policy_initial)):\n",
    "    policy_initial[i]=nprand.choice([1,2,3])\n",
    "    p_rand1[i]=nprand.choice([1,2,3])\n",
    "    p_rand2[i]=nprand.choice([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  3.,  2.,  1.,  1.,  3.,  2.,  3.,  1.,  2.,  1.,  1.,  3.,\n",
       "        3.,  3.,  2.,  2.,  1.,  2.,  1.,  1.,  1.,  3.,  3.,  1.,  2.,\n",
       "        3.,  3.,  2.,  2.,  1.,  3.,  1.,  1.,  2.,  3.,  1.,  1.,  1.,\n",
       "        1.,  1.,  2.,  2.,  1.,  1.,  3.,  3.,  1.,  2.,  3.,  1.,  3.,\n",
       "        2.,  2.,  3.,  1.,  1.,  3.,  3.,  1.,  3.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_initial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the find_new_state function, which takes as argument a policy p and an index i. The index i represents the state we are in actually and the function return in which state we will be if we follow the policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_new_state(p,i):\n",
    "\n",
    "# we find the action the policy tells us to take then we scan the corresponding matrix to find the resulting state\n",
    "# if the state is a \"low speed state\" and we choose to decelerate then we stay where we are, hence return i\n",
    "    \n",
    "    k=p[i]\n",
    "    \n",
    "    if k==1:\n",
    "        if i%3 == 0:\n",
    "            return i\n",
    "        else :\n",
    "            for j in range(len(T_decelerate[i])):\n",
    "                if  j != i and T_decelerate[i][j]==1:\n",
    "                    return  j    \n",
    "    elif k ==2 :\n",
    "        for j in range(len(T_maintain[i])):\n",
    "            if  j != i and T_maintain[i][j]==1:\n",
    "                return j\n",
    "            return j\n",
    "                \n",
    "    elif k==3:\n",
    "        for j in range(len(T_accel[i])):\n",
    "            if  j != i and T_accel[i][j]==1:\n",
    "                return j\n",
    "            return j\n",
    "\n",
    "    else :\n",
    "         raise ValueError(\"k should be between 1 and 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problème de modélisation de l'environnement : la voiture a techniquement la possibilité de \"dépasser\" L, j'ai pas encore trouvé de bonne façon d'empêcher ça... Une bonnne façon peut être est juste de modifier la fonction find new state d'une façon plus optimale (test i-i%3 --> position puis check vitesse puis check action pour trouve position future et ensuite se débrouiller)\n",
    "\n",
    "Ou sinon rajouter un autre état, celui dans lequel il ne faudrait surtout pas aller et laisser tout le monde à -1, sauf L à +1 et lui à -10*3*l ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests while modeling everything : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the policy iteration procedure to figure out the best policy to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#definition of  stopping criterion \n",
    "epsilon = 0.01\n",
    "#definition of discount factor\n",
    "gamma=0.9\n",
    "#definitions of the states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this policy evaluation is only working for non-stochastic policies and is an in-place one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2010"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lava"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def policy_eval(p,epsilon):\n",
    "    delta = 10\n",
    "    V=np.zeros(3*l+1)\n",
    "    while(delta > epsilon):\n",
    "        delta = 0\n",
    "        for i in range(3*l+1):\n",
    "            v=V[i]\n",
    "            j=find_new_state(p,i)\n",
    "            V[i]=R[j]+gamma*V[j]\n",
    "            delta = max(delta,abs(v-V[i]))\n",
    "\n",
    "        \n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "checking if policy_eval() is not going completely crazy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20275.71175750051"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LA.norm(policy_eval(policy_initial,0.01)-policy_eval(p_rand1,0.01))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color='red'>\n",
    "policy_improv is to be improved, but I don't know the exact command to do what I want, so it's a beginning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def policy_improv(p,V,boolean):\n",
    "    new_p=np.zeros(shape=3*l+1)\n",
    "    \n",
    "    for i in range(3*l+1):\n",
    "        for j in range(len(T_maintain[i])):\n",
    "            if  j != i and T_maintain[i][j]==1:\n",
    "                k=j\n",
    "        action_1=R[k]+gamma*V[k]\n",
    "        \n",
    "        \n",
    "        for j in range(len(T_accel[i])):\n",
    "            if  j != i and T_accel[i][j]==1:\n",
    "                k=j\n",
    "        action_2=R[k]+gamma*V[k]\n",
    "        \n",
    "        \n",
    "        for j in range(len(T_decelerate[i])):\n",
    "            if T_decelerate[i][j]==1:\n",
    "                k=j\n",
    "        action_3=R[k]+gamma*V[k]\n",
    "        \n",
    "        new_p[i]=np.argmax([action_1,action_2,action_3])+1\n",
    "        if new_p[i]!=p[i]:\n",
    "            boolean = False\n",
    "    return new_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now combine the two functions to iterate over policies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# policy iteration\n",
    "p=p_test\n",
    "Delta_iter=0.01\n",
    "epsilon_iter=0.001\n",
    "policy_stable=False\n",
    "\n",
    "while  not policy_stable :\n",
    "    V=policy_eval(p,epsilon)\n",
    "    policy_stable= True\n",
    "    p=policy_improv(p,V,policy_stable)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -9, -20099, -9, -9, -20099, -9, -20099, -20099, -20099]\n"
     ]
    }
   ],
   "source": [
    "print([int(elem) for elem in V])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  3.,  1.,  1.,  1.,  3.,  3.,  2.,  1.,  1.,  3.,  1.,  1.,\n",
       "        3.,  1.,  1.,  1.,  3.,  2.,  1.,  3.,  1.,  2.,  1.,  1.,  3.,\n",
       "        1.,  1.,  3.,  1.,  1.,  1.,  1.,  3.,  1.,  1.,  1.,  3.,  1.,\n",
       "        1.,  1.,  3.,  3.,  1.,  3.,  1.,  3.,  3.,  1.,  3.,  3.,  1.,\n",
       "        3.,  1.,  1.,  3.,  1.,  3.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  1.,  1.,  1.,  3.,  1.,  1.,  1.,  3.,  2.,  1.,  1.,  1.,\n",
       "        3.,  1.,  1.,  2.,  1.,  1.,  1.,  3.,  3.,  1.,  1.,  1.,  1.,\n",
       "        3.,  1.,  2.,  1.,  1.,  3.,  1.,  1.,  2.,  1.,  2.,  1.,  3.,\n",
       "        1.,  3.,  1.,  1.,  1.,  3.,  3.,  1.,  3.,  1.,  3.,  3.,  1.,\n",
       "        3.,  1.,  1.,  3.,  1.,  3.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_test=p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To figure out if everything is going well, make sure that at each iteration you keep track of the value vector, as well as the trajectory of the car according to the current policy. The latter allows you to compute the current policy's total reward and plot the evolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use the stored values to make a video similar to _street_racer.mp4_ on the repo. The following procedure can be used to save figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-b9c7be38faac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtrap\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m72\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m111\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'nearest'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'values' is not defined"
     ]
    }
   ],
   "source": [
    "for idx, v in enumerate(values):\n",
    "    v = np.array(v[:trap]).reshape(3, l)\n",
    "    fig = plt.figure(figsize=(l*2, 6), dpi=72)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(v, interpolation='nearest', cmap='gray')\n",
    "    plt.yticks([])\n",
    "    plt.savefig('img/value_'+str(idx)+'.jpg', dpi=72, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the command-line utility _ffmpeg_ and use it to transform the saved sequence of images into a mp4 video.\n",
    "\n",
    "(https://en.wikibooks.org/wiki/FFMPEG_An_Intermediate_Guide/image_sequence#Making_a_video_from_an_Image_Sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with your model. What happens if you introduce uncertainty about the car's brakes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
