{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as nprand\n",
    "from numpy import linalg as LA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jupyternotify extension is already loaded. To reload it, use:\n",
      "  %reload_ext jupyternotify\n"
     ]
    }
   ],
   "source": [
    "load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Street Racer\n",
    "\n",
    "In this notebook, you'll apply the methods of chapter 4 of Sutton's book to a simple racing problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem consists in driving a car as fast of possible over an exact distance $L$, and stopping there.\n",
    "\n",
    "This distance is divided in steps $0, ..., L$. The car can drive at three different speed: _low_, _medium_, _high_. Leaving step $j$ at _low_ speed, it will move to $j+1$. _Medium_ and _high_ bring it to $j+2$ and $j+3$, respectively.\n",
    "\n",
    "At any step, the driver can decide to _decelerate_, _maintain speed_ or _accelerate_. Decelarating will cause the car to leave its current place at one speed lower. If the car is already at _low_ speed, decelarating keeps it in the same spot. Maintaining speed does exactly what you think. Accelerating will increase the speed by one, except at _high_ speed, where it is equivalent to maintaining speed.\n",
    "\n",
    "The car starts on step $0$ at _low_ speed.\n",
    "\n",
    "Beyond the $L$ distance there is a huge, hot lake of lava. Needless to say, the car must be able to stop at $L$, or the driver will suffer quite a lot."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    car ->                    ||/\\/\\/\n",
    "     |_______________ ... ____||/\\/ lava\n",
    "     |    |    |    |       | ||/\\/\\/\n",
    "     0    1    2    3       L ||/\\/\\/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help the driver win the race and not die, build a model of the problem and apply the policy iteration and value iteration methods to find her optimal trajectory.\n",
    "\n",
    "As this problem is an (over-)simplification of our traffic light problem, any work done here could serve as a building block for later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by figuring out the number of states you will need and build transition matrices for every action. For now, actions move the car from state to state in the deterministic manner described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = 20\n",
    "n = 3*l+1\n",
    "trap=n-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "T_decelerate = np.zeros(shape=(n, n))\n",
    "T_maintain = np.zeros(shape=(n,n))\n",
    "T_accel = np.zeros(shape=(n,n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we have three states for each position then we have a lava state (absorbing state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the matrix repeats itself for each action :  (here the decelerate matrix)\n",
    "\n",
    "\\begin{pmatrix}1&0&0 & 0&0&0 & 0&0&0 &  0&0&0 & \\ldots \n",
    "\\\\ 0&0&0 & 1&0&0 & 0&0&0 &  0&0&0 & \\ldots \n",
    "\\\\ 0&0&0 & 0&0&0 & 0&1&0 &  0&0&0 & \\ldots\n",
    "\\\\ 0&0&0 & 1&0&0 & 0&0&0 &  0&0&0 & \\ldots\n",
    "\\\\ 0&0&0 & 0&0&0 & 1&0&0 &  0&0&0 & \\ldots\n",
    "\\\\ 0&0&0 & 0&0&0 & 0&0&0 &  0&1&0 & \\ldots\n",
    "\\end{pmatrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(n-1):\n",
    "    if (i%3 == 0 ):\n",
    "        T_decelerate[i][i] = 1        #if at low speed and decelerating : stay on the same spot at low speed\n",
    "    if (i%3 == 1):                \n",
    "        if (i+2 > n-1):\n",
    "            T_decelerate[i][n-1] = 1  #if action takes us beyond l : we fall into the lava-state  \n",
    "        else :     \n",
    "            T_decelerate[i][i+2] = 1  #other wise we just end up in the next position with low speed since we are in medium \n",
    "    if (i%3 == 2):\n",
    "        if (i+5 > n-1):\n",
    "            T_decelerate[i][n-1] = 1  #if action takes us beyond l : we fall into the lava-state \n",
    "        else : \n",
    "            T_decelerate[i][i+5] = 1  #otherwise we just end up two states after with medium speed \n",
    "                       \n",
    "\n",
    "for i in range(n-1):\n",
    "    if (i%3 == 0):\n",
    "        if (i+7 > n-1):\n",
    "            T_accel[i][n-1] = 1\n",
    "        else :\n",
    "            T_accel[i][i+7] = 1\n",
    "        if (i+3 > n-1 ):\n",
    "            T_maintain[i][n-1] = 1\n",
    "        else:\n",
    "            T_maintain[i][i+3] = 1\n",
    "    if (i%3 == 1):\n",
    "        if (i+10 > n-1):\n",
    "            T_accel[i][n-1] = 1\n",
    "        else : \n",
    "            T_accel[i][i+10] = 1\n",
    "        if (i+6 > n-1):\n",
    "            T_maintain[i][n-1] = 1\n",
    "        else :\n",
    "            T_maintain[i][i+6] = 1\n",
    "    if (i%3 == 2):\n",
    "        if(i+9 > n-1):\n",
    "            T_accel[i][n-1] = 1\n",
    "            T_maintain[i][n-1] = 1\n",
    "        else :\n",
    "            T_accel[i][i+9] = 1\n",
    "            T_maintain[i][i+9] = 1\n",
    "\n",
    "            \n",
    "#once dead you stay dead !            \n",
    "T_accel[n-1][n-1] = 1\n",
    "T_maintain[n-1][n-1] = 1\n",
    "T_decelerate[n-1][n-1] = 1 \n",
    "\n",
    "\n",
    "#define L_low as an absorbing state\n",
    "T_accel[-4] = np.zeros(3*l+1)\n",
    "T_decelerate[-4] = np.zeros(3*l+1)\n",
    "T_maintain[-4] = np.zeros(3*l+1)\n",
    "\n",
    "T_accel[-4][-4] = 1\n",
    "T_maintain[-4][-4] = 1\n",
    "T_decelerate[-4][-4] = 1\n",
    "\n",
    "\n",
    "#definition of the actual matrix \n",
    "\n",
    "T = np.zeros(shape = (3,n,n))\n",
    "T[0] = T_decelerate\n",
    "T[1] = T_maintain\n",
    "T[2] = T_accel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we check for any encoding issues in the matrices : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def error_matrix(A):\n",
    "    result = np.dot(A,np.ones(len(A)))\n",
    "    print(\"erreur de type pas d'actions associées \",np.where(result == 0)[0])\n",
    "    print(\"erreur de type deux actions ou plus associées \",np.where(result >= 2)[0],\":\")\n",
    "    for i in np.where(result >= 2)[0]:\n",
    "        print(\"sum of the line : \",result[i])\n",
    "        print(A[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deceleration matrix\n",
      "erreur de type pas d'actions associées  []\n",
      "erreur de type deux actions ou plus associées  [] :\n",
      "acceleration matrix\n",
      "erreur de type pas d'actions associées  []\n",
      "erreur de type deux actions ou plus associées  [] :\n",
      "maintain matrix\n",
      "erreur de type pas d'actions associées  []\n",
      "erreur de type deux actions ou plus associées  [] :\n"
     ]
    }
   ],
   "source": [
    "print(\"deceleration matrix\")\n",
    "error_matrix(T_decelerate)\n",
    "print(\"acceleration matrix\")\n",
    "error_matrix(T_accel)\n",
    "print(\"maintain matrix\")\n",
    "error_matrix(T_maintain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And define the reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "R = -np.ones(n)\n",
    "lava = -l\n",
    "R[-1] =  lava  #lava state : you die\n",
    "R[-4] = l       #L in low speed ! win ! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining a policy : at the beginning we choose a random policy. A policy is here encoded as a sequence of letters : 0,1,2 for decelerate, maintain, accelerate. The sequence is of length n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "policy_initial = np.zeros(n,dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the find_new_state function, which takes as argument a policy p and an index i. The index i represents the state we are in actually and the function return in which state we will be if we follow the policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_new_state(p,i,T):\n",
    "\n",
    "    array = np.where(T[int(p[i])][i] > 0)[0]\n",
    "    if len(array) == 1:\n",
    "        return int(array)\n",
    "    else :\n",
    "        return int(np.randchoice(array))   ##to be modified with the probability : maybe can already be done ?\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = find_new_state(policy_initial,0,T)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the policy iteration procedure to figure out the best policy to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#definition of  stopping criterion \n",
    "epsilon = 0.01\n",
    "#definition of discount factor\n",
    "gamma = 0.9\n",
    "#definitions of the states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this policy evaluation is only working for non-stochastic policies and is an in-place one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def policy_eval(p, T, R, threshold, gamma):\n",
    "    delta = 10\n",
    "    V=np.zeros(n)\n",
    "    iteration_number = 0\n",
    "    \n",
    "    while(delta >= threshold and iteration_number < 1000):  #iteration number in order to not kill my computer\n",
    "        delta = 0\n",
    "        iteration_number+=1\n",
    "        for i in range(len(T[0])):  #iteration on all the states of departure\n",
    "            old_value = V[i]\n",
    "            new_value = 0\n",
    "            a=int(p[i])\n",
    "            for s in range(len(T[0])): #iteration on all the states of arrival\n",
    "                new_value += T[a][i][s]*(R[s]+gamma*V[s])  #computation of v(s)\n",
    "            V[i] = new_value\n",
    "            delta = max(delta,abs(old_value-V[i]))\n",
    "        \n",
    "        \n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  -9.99588902,   -9.99588902,   -9.99588902,   -9.99588902,\n",
       "         -9.99588902,   -9.99588902,   -9.99588902,   -9.99588902,\n",
       "         -9.99588902,   -9.99588902,   -9.99588902,   -9.99588902,\n",
       "         -9.99588902,   -9.99588902,   -9.99588902,   -9.99588902,\n",
       "         -9.99588902,   -9.99588902,   -9.99588902,   -9.99588902,\n",
       "         -9.99588902,   -9.99588902,   -9.99588902,   -9.99588902,\n",
       "         -9.99588902,   -9.99588902,   -9.99588902,   -9.99588902,\n",
       "         -9.99588902,   -9.99588902,   -9.99588902,   -9.99588902,\n",
       "         -9.99588902,   -9.99588902,   -9.99588902,   -9.99588902,\n",
       "         -9.99588902,   -9.99588902,   -9.99588902,   -9.99588902,\n",
       "         -9.99588902,   -9.99588902,   -9.99588902,   -9.99588902,\n",
       "         -9.99588902,   -9.99588902,   -9.99588902,   -9.99588902,\n",
       "         -9.99588902,   -9.99588902,  178.91778034,   -9.99588902,\n",
       "         -9.99588902, -180.91778034,   -9.99588902,  199.91778034,\n",
       "       -199.91778034,  199.91778034, -199.91778034, -199.91778034,\n",
       "       -199.91778034])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_eval(policy_initial,T,R,0.01,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "A=[0,1,2]   #definition of the differents actions possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def policy_improv(p,V,T,R,A,gamma,epsilon,stability):\n",
    "    \n",
    "    new_p=np.zeros(shape=n,dtype=int)\n",
    "    stability = True\n",
    "    \n",
    "    for i in range(len(V)): #iteration on all the states\n",
    "        \n",
    "        old_action=p[i]\n",
    "        vals_possible=[]\n",
    "        \n",
    "        for a in A : #iterations on all the possibles action\n",
    "            new_value = 0\n",
    "            \n",
    "            for s in range(len(V)):  #iterations on all the possible states of arrival \n",
    "                new_value += T[a][i][s]*(R[s]+gamma*V[s])\n",
    "            vals_possible.append(new_value)\n",
    "            \n",
    "        new_p[i]=np.argmax(vals_possible)\n",
    "        if old_action != new_p[i]:\n",
    "            stability = False\n",
    "    \n",
    "    return new_p,stability  \n",
    "\n",
    "# maybe not exactly our algorithm : we always have the same set of actions for each states in our case but what about \n",
    "# the more general case ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def state(i):\n",
    "    \n",
    "# return the position and the state we are in\n",
    "    \n",
    "    if i%3 == 0 :\n",
    "        return ((i-i%3)/3,\"low\")\n",
    "    elif i%3 == 1 :\n",
    "        return ((i-i%3)/3,\"medium\")\n",
    "    elif i%3 == 2 : \n",
    "        return ((i-i%3)/3,\"high\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tracker(p,R,T):\n",
    "    \n",
    "# keeps track of the itinerary : state + actual velocity we go throught\n",
    "#                    actions : the actions that were taken \n",
    "#                    scores : the reward we get along the way \n",
    "\n",
    "\n",
    "    itinerary = []\n",
    "    action = []\n",
    "    scores = []\n",
    "    itinerary.append((0,\"low\"))\n",
    "    action.append(p[0])\n",
    "    scores=R[0]\n",
    "    s = 0\n",
    "    \n",
    "    it = 0\n",
    "    \n",
    "    while(it <l-1):\n",
    "        action.append(p[s])\n",
    "        s=find_new_state(p,s,T)\n",
    "        #print(s)\n",
    "        itinerary.append(state(s))\n",
    "        scores += R[s]\n",
    "        it += 1\n",
    "        if (state(s)[0] >= l-1):\n",
    "            break\n",
    "    \n",
    "    return action, itinerary, scores\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now combine the two functions to iterate over policies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "$(document).ready(\n",
       "    function() {\n",
       "        function appendUniqueDiv(){\n",
       "            // append a div with our uuid so we can check that it's already\n",
       "            // been sent and avoid duplicates on page reload\n",
       "            var notifiedDiv = document.createElement(\"div\")\n",
       "            notifiedDiv.id = \"31a341dc-2e7d-44c5-bc02-007dedf649b3\"\n",
       "            element.append(notifiedDiv)\n",
       "        }\n",
       "\n",
       "        // only send notifications if the pageload is complete; this will\n",
       "        // help stop extra notifications when a saved notebook is loaded,\n",
       "        // which during testing gives us state \"interactive\", not \"complete\"\n",
       "        if (document.readyState === 'complete') {\n",
       "            // check for the div that signifies that the notification\n",
       "            // was already sent\n",
       "            if (document.getElementById(\"31a341dc-2e7d-44c5-bc02-007dedf649b3\") === null) {\n",
       "                var notificationPayload = {\"requireInteraction\": false, \"icon\": \"/static/base/images/favicon.ico\", \"body\": \"Cell execution has finished!\"};\n",
       "                if (Notification.permission !== 'denied') {\n",
       "                    if (Notification.permission !== 'granted') { \n",
       "                        Notification.requestPermission(function (permission) {\n",
       "                            if(!('permission' in Notification)) {\n",
       "                                Notification.permission = permission\n",
       "                            }\n",
       "                        })\n",
       "                    }\n",
       "                    if (Notification.permission === 'granted') {\n",
       "                    var notification = new Notification(\"Jupyter Notebook\", notificationPayload)\n",
       "                    appendUniqueDiv()\n",
       "                    notification.onclick = function () {\n",
       "                        window.focus();\n",
       "                        this.close();\n",
       "                        };\n",
       "                    } \n",
       "                }     \n",
       "            }\n",
       "        }\n",
       "    }\n",
       ")\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# policy iteration\n",
    "p = policy_initial\n",
    "policy_stable = False\n",
    "gamma = 0.9\n",
    "epsilon = 0.01\n",
    "iterations = 0\n",
    "scores = []\n",
    "\n",
    "while not policy_stable and iterations < 100 :\n",
    "    #print(\"potatoes\")\n",
    "    V = policy_eval(p,T,R,epsilon,gamma)\n",
    "    p, policy_stable = policy_improv(p,V,T,R,A,gamma,epsilon,policy_stable)\n",
    "    iterations +=1\n",
    "    scores.append(tracker(p,R,T)[2])\n",
    "    \n",
    "    \n",
    "%notify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4ANAThD4AJATBD4A5MT/A0B1Y3vZF+jMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x24269eaf828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.0"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tracker(p,R,T)[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "act, iti, scores = tracker(p,R,T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 2, 1, 2, 1, 1, 1, 0, 0]\n",
      "[(0, 'low'), (2.0, 'medium'), (4.0, 'medium'), (7.0, 'high'), (10.0, 'high'), (13.0, 'high'), (16.0, 'high'), (18.0, 'medium'), (19.0, 'low')]\n",
      "12.0\n"
     ]
    }
   ],
   "source": [
    "print(act)\n",
    "print(iti)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.\n",
      "  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.\n",
      "  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.\n",
      "  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  -1.  20.  -1.  -1.\n",
      " -20.]\n"
     ]
    }
   ],
   "source": [
    "print(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "values =V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To figure out if everything is going well, make sure that at each iteration you keep track of the value vector, as well as the trajectory of the car according to the current policy. The latter allows you to compute the current policy's total reward and plot the evolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use the stored values to make a video similar to _street_racer.mp4_ on the repo. The following procedure can be used to save figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "invalid index to scalar variable.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-b9c7be38faac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtrap\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m72\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m111\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'nearest'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
     ]
    }
   ],
   "source": [
    "for idx, v in enumerate(values):\n",
    "    v = np.array(v[:trap]).reshape(3, l)\n",
    "    fig = plt.figure(figsize=(l*2, 6), dpi=72)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(v, interpolation='nearest', cmap='gray')\n",
    "    plt.yticks([])\n",
    "    plt.savefig('img/value_'+str(idx)+'.jpg', dpi=72, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the command-line utility _ffmpeg_ and use it to transform the saved sequence of images into a mp4 video.\n",
    "\n",
    "(https://en.wikibooks.org/wiki/FFMPEG_An_Intermediate_Guide/image_sequence#Making_a_video_from_an_Image_Sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with your model. What happens if you introduce uncertainty about the car's brakes?"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
