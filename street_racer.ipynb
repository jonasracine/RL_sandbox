{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.random as nprand"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Street Racer\n",
    "\n",
    "In this notebook, you'll apply the methods of chapter 4 of Sutton's book to a simple racing problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem consists in driving a car as fast of possible over an exact distance $L$, and stopping there.\n",
    "\n",
    "This distance is divided in steps $0, ..., L$. The car can drive at three different speed: _low_, _medium_, _high_. Leaving step $j$ at _low_ speed, it will move to $j+1$. _Medium_ and _high_ bring it to $j+2$ and $j+3$, respectively.\n",
    "\n",
    "At any step, the driver can decide to _decelerate_, _maintain speed_ or _accelerate_. Decelarating will cause the car to leave its current place at one speed lower. If the car is already at _low_ speed, decelarating keeps it in the same spot. Maintaining speed does exactly what you think. Accelerating will increase the speed by one, except at _high_ speed, where it is equivalent to maintaining speed.\n",
    "\n",
    "The car starts on step $0$ at _low_ speed.\n",
    "\n",
    "Beyond the $L$ distance there is a huge, hot lake of lava. Needless to say, the car must be able to stop at $L$, or the driver will suffer quite a lot."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    car ->                    ||/\\/\\/\n",
    "     |_______________ ... ____||/\\/ lava\n",
    "     |    |    |    |       | ||/\\/\\/\n",
    "     0    1    2    3       L ||/\\/\\/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To help the driver win the race and not die, build a model of the problem and apply the policy iteration and value iteration methods to find her optimal trajectory.\n",
    "\n",
    "As this problem is an (over-)simplification of our traffic light problem, any work done here could serve as a building block for later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by figuring out the number of states you will need and build transition matrices for every action. For now, actions move the car from state to state in the deterministic manner described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "T_decelerate = np.zeros(shape=(3*l, 3*l))\n",
    "T_maintain = np.zeros(shape=(3*l, 3*l))\n",
    "T_accel = np.zeros(shape=(3*l, 3*l))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the matrix repeats itself for each action :  (here the decelerate matrix)\n",
    "\n",
    "\\begin{pmatrix}1&0&0 & 0&0&0 & 0&0&0 &  0&0&0 & \\ldots \n",
    "\\\\ 0&0&0 & 1&0&0 & 0&0&0 &  0&0&0 & \\ldots \n",
    "\\\\ 0&0&0 & 0&0&0 & 0&1&0 &  0&0&0 & \\ldots\n",
    "\\\\ 0&0&0 & 1&0&0 & 0&0&0 &  0&0&0 & \\ldots\n",
    "\\\\ 0&0&0 & 0&0&0 & 1&0&0 &  0&0&0 & \\ldots\n",
    "\\\\ 0&0&0 & 0&0&0 & 0&0&0 &  0&1&0 & \\ldots\n",
    "\\end{pmatrix}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(len(T_decelerate)-5):\n",
    "    if (i%3==0 ):\n",
    "        T_decelerate[i][i]=1\n",
    "    if (i%3==1):\n",
    "        T_decelerate[i][i+2]=1\n",
    "    if (i%3==2):\n",
    "        T_decelerate[i][i+5]=1\n",
    "\n",
    "n=len(T_decelerate)\n",
    "T_decelerate[n-5][n-3]=1\n",
    "T_decelerate[n-3][n-3]=1      \n",
    "\n",
    "\n",
    "for i in range(len(T_accel)-10):\n",
    "    if (i%3==0):\n",
    "        T_accel[i][i+7]=1\n",
    "        T_maintain[i][i+3]=1\n",
    "    if (i%3==1):\n",
    "        T_accel[i][i+10]=1\n",
    "        T_maintain[i][i+6]=1\n",
    "    if (i%3==2):\n",
    "        T_accel[i][i+9]=1\n",
    "        T_maintain[i][i+9]=1\n",
    "        \n",
    "m=len(T_accel)\n",
    "j=len(T_maintain)\n",
    "\n",
    "T_accel[m-10][m-1]=1\n",
    "T_accel[m-9][m-2]=1\n",
    "\n",
    "T_maintain[j-10][j-1]=1\n",
    "T_maintain[j-9][j-6]=1\n",
    "T_maintain[j-8][j-2]=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And define the reward function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "R = -np.ones(3*l)\n",
    "R[len(R)-1]=-1000^l\n",
    "R[len(R)-2]=-1000^l\n",
    "R[len(R)-4]=-1000^l\n",
    "R[len(R)-7]=-1000^l\n",
    "\n",
    "R[len(R)-3]=10000^l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defining a policy : at the beginning we choose a random policy. A policy is here encoded as a sequence of letters : 1,2,3 for maintain, accelerate, decelerate. The sequence is of length l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "policy_initial = np.zeros(3*l)\n",
    "for i in range(len(policy_initial)):\n",
    "    policy_initial[i]=nprand.choice([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  3.,  2.,  2.,  3.,  3.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_initial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the find_new_state function, which takes as argument a policy p and an index i. The index i represents the state we are in actually and the function return in which state we will be if we follow the policy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_new_state(p,i):\n",
    "    k=p[i]\n",
    "    if k ==1 :\n",
    "        for j in range(len(T_maintain[i])):\n",
    "            j=3*l-1 ##si la voiture dépasse le point L, on la force à être au point L avec vitesse high\n",
    "            if  j != i and T_maintain[i][j]==1:\n",
    "                return j\n",
    "            return j\n",
    "                \n",
    "    elif k==2:\n",
    "        j=3*l-1  ##si la voiture dépasse le point L, on la force à être au point L avec vitesse high\n",
    "        for j in range(len(T_accel[i])):\n",
    "            if  j != i and T_accel[i][j]==1:\n",
    "                return j\n",
    "            return j\n",
    "    elif k==3:\n",
    "        retour = i;\n",
    "        for j in range(len(T_decelerate[i])):\n",
    "            if  j != i and T_decelerate[i][j]==1:\n",
    "                retour = j\n",
    "        return retour\n",
    "    else :\n",
    "         raise ValueError(\"k should be between 1 and 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problème de modélisation de l'environnement : la voiture a techniquement la possibilité de \"dépasser\" L, j'ai pas encore trouvé de bonne façon d'empêcher ça... Une bonnne façon peut être est juste de modifier la fonction find new state d'une façon plus optimale (test i-i%3 --> position puis check vitesse puis check action pour trouve position future et ensuite se débrouiller)\n",
    "\n",
    "Ou sinon rajouter un autre état, celui dans lequel il ne faudrait surtout pas aller et laisser tout le monde à -1, sauf L à +1 et lui à -10*3*l ? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tests while modeling everything : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_new_state(policy_initial,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_initial[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(T_decelerate[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply the policy iteration procedure to figure out the best policy to follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#definition of  stopping criterion \n",
    "epsilon = 0.01\n",
    "#definition of discount factor\n",
    "gamma=0.5\n",
    "#definitions of the states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this policy evaluation is only working for non-stochastic policies and is an in-place one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def policy_eval(p,epsilon):\n",
    "    delta = 10\n",
    "    V=np.zeros(3*l)\n",
    "    while(delta >= epsilon):\n",
    "        delta = 0\n",
    "        for i in range(3*l):\n",
    "            v=V[i]\n",
    "            j=find_new_state(p,i)\n",
    "            V[i]=R[j]+gamma*V[j]\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(find_new_state(policy_initial,52))\n",
    "#policy_initial[52]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -9.97000000e+02,  -1.00000000e+00,  -4.99500000e+02,\n",
       "        -4.99500000e+02,   1.00030000e+04,  -9.97000000e+02,\n",
       "        -9.97000000e+02,  -9.97000000e+02,  -9.97000000e+02])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_eval(policy_initial,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <font color='red'>\n",
    "policy_improv is to be improved, but I don't know the exact command to do what I want, so it's a beginning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def policy_improv(p,V,boolean):\n",
    "    new_p=np.zeros(shape=3*l)\n",
    "    for i in range(3*l):\n",
    "        for j in range(len(T_maintain[i])):\n",
    "            if  j != i and T_maintain[i][j]==1:\n",
    "                k=j\n",
    "        action_1=R[k]+gamma*V[k]\n",
    "        for j in range(len(T_accel[i])):\n",
    "            if  j != i and T_accel[i][j]==1:\n",
    "                k=j\n",
    "        action_2=R[k]+gamma*V[k]\n",
    "        for j in range(len(T_decelerate[i])):\n",
    "            if T_decelerate[i][j]==1:\n",
    "                k=j\n",
    "        action_3=R[k]+gamma*V[k]\n",
    "        new_p[i]=np.argmax([action_1,action_2,action_3])+1\n",
    "        if new_p[i]!=p[i]:\n",
    "            boolean = False\n",
    "    return new_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now combine the two functions to iterate over policies!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# policy iteration\n",
    "p=policy_initial\n",
    "Delta_iter=1\n",
    "epsilon_iter=0.1\n",
    "policy_stable=False\n",
    "\n",
    "while  not policy_stable :\n",
    "    V=policy_eval(p,epsilon)\n",
    "    policy_stable= True\n",
    "    p=policy_improv(p,V,policy_stable)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ -9.97000000e+02,  -1.00000000e+00,  -4.99500000e+02,\n",
       "        -4.99500000e+02,   1.00030000e+04,  -9.97000000e+02,\n",
       "        -9.97000000e+02,  -9.97000000e+02,  -9.97000000e+02])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  3.,  1.,  3.,  3.,  1.,  1.,  1.,  1.])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To figure out if everything is going well, make sure that at each iteration you keep track of the value vector, as well as the trajectory of the car according to the current policy. The latter allows you to compute the current policy's total reward and plot the evolution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then use the stored values to make a video similar to _street_racer.mp4_ on the repo. The following procedure can be used to save figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'values' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-96-b9c7be38faac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtrap\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m72\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m111\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minterpolation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'nearest'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'gray'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'values' is not defined"
     ]
    }
   ],
   "source": [
    "for idx, v in enumerate(values):\n",
    "    v = np.array(v[:trap]).reshape(3, l)\n",
    "    fig = plt.figure(figsize=(l*2, 6), dpi=72)\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(v, interpolation='nearest', cmap='gray')\n",
    "    plt.yticks([])\n",
    "    plt.savefig('img/value_'+str(idx)+'.jpg', dpi=72, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the command-line utility _ffmpeg_ and use it to transform the saved sequence of images into a mp4 video.\n",
    "\n",
    "(https://en.wikibooks.org/wiki/FFMPEG_An_Intermediate_Guide/image_sequence#Making_a_video_from_an_Image_Sequence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Play around with your model. What happens if you introduce uncertainty about the car's brakes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
