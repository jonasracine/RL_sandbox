                             %%%%      Books       %%%%



@book{Sutton,
 author = {Sutton, Richard S. and Barto, Andrew G.},
 title = {Introduction to Reinforcement Learning},
 year = {1998},
 isbn = {0262193981},
 edition = {1st},
 publisher = {MIT Press},
 address = {Cambridge, MA, USA},
} 
@book{}
@book{Puterman,
 author = {Puterman, Martin L.},
 title = {Markov Decision Processes: Discrete Stochastic Dynamic Programming},
 year = {1994},
 isbn = {0471619779},
 edition = {1st},
 publisher = {John Wiley \& Sons, Inc.},
 address = {New York, NY, USA},
} 





						%%% PHD thesis %%% 
@PHDTHESIS{}
@PHDTHESIS{Xia2015,
url = {http://www.theses.fr/2015ECLI0026},
title = {Apprentissage Intelligent des Robots Mobiles dans la Navigation Autonome},
author = {Xia, Chen},
year = {2015},
note = {Th\`{e}se de doctorat dirig\'{e}e par El Kamel, Abdelkader Automatique, g\'{e}nie informatique, traitement du signal et des images Ecole centrale de Lille 2015},
note = {2015ECLI0026},
url = {http://www.theses.fr/2015ECLI0026/document},
}

@PhdThesis{Watkins_phd,
  author =       {Watkins, Christopher John Cornish Hellaby},
  title =        {Learning from Delayed Rewards},
  school =       {King's College},
  year =         {1989},
  address =   {Cambridge, UK},
  month =     {May},
}


				%%% ???? %%%% 
@MISC{CS287 ,
    title={Advanced {R}obotics, {UC} {B}erkeley {EECS}},
    year={2009},
    howpublished={https://people.eecs.berkeley.edu/~pabbeel/cs287-fa09},
    
}

				%%%articles%%%


@article{KLMSurvey,
    author = "Leslie Pack Kaelbling and Michael L. Littman and Andrew P. Moore",
    title = "Reinforcement Learning: A Survey",
    journal = "Journal of Artificial Intelligence Research",
    volume = "4",
    pages = "237-285",
    year = "1996",
url={http://people.csail.mit.edu/lpk/papers/rl-survey.ps}}
\\
@Article{Watkins_article,
author="Watkins, Christopher J. C. H.
and Dayan, Peter",
title="Q-learning",
journal="Machine Learning",
year="1992",
month="May",
day="01",
volume="8",
number="3",
pages="279--292",
abstract="Q-learning (Watkins, 1989) is a simple way for agents to learn how to act optimally in controlled Markovian domains. It amounts to an incremental method for dynamic programming which imposes limited computational demands. It works by successively improving its evaluations of the quality of particular actions at particular states.",
issn="1573-0565",
doi="10.1007/BF00992698",
url="https://doi.org/10.1007/BF00992698"
}


@techreport{rummery,
  added-at = {2008-03-11T14:52:34.000+0100},
  address = {Cambridge, England},
  author = {Rummery, G. A. and Niranjan, M.},
  biburl = {https://www.bibsonomy.org/bibtex/220239f82583859588c96171ccc015e65/idsia},
  citeulike-article-id = {2378892},
  institution = {Cambridge University Engineering Department},
  interhash = {0c7cd3821ad0fe1b39a6ce1b35ec4bc0},
  intrahash = {20239f82583859588c96171ccc015e65},
  keywords = {nn},
  number = {TR 166},
  priority = {2},
  timestamp = {2008-03-11T14:59:39.000+0100},
  title = {On-Line {Q}-Learning Using Connectionist Systems},
  year = 1994
}


