\documentclass[dvipsnames,svgnames]{beamer}
\usetheme{Warsaw}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{tikz}
\usetikzlibrary{automata, positioning}
\usepackage{float}
\usepackage[normalem]{ulem}





\author{Charles Dufour}
\title{Reinforcement learning and robot navigation }
%\setbeamercovered{transparent} 
\setbeamertemplate{navigation symbols}{} 
%\logo{} 
%\institute{} 
\date{May 2, 2018}
%\subject{} 

\newtheorem{madef}{Definition}

\begin{document}


\begin{frame}
\titlepage
\centering
\includegraphics[scale=1]{img/Wall-E.jpg}
\end{frame}

%\begin{frame}
%\tableofcontents
%\end{frame}



\begin{frame}
\frametitle{Introduction}
\begin{block}{The problem}

  \begin{itemize}
   \item Framework: a raspberry pie 3 robot which can follow lines
   \item The task: the robot should adapt its speed with respect to traffic lights
   \item How: using Reinforcement Learning (RL) and Markov Decision Process (MDP)  
  \end{itemize}
\end{block} 
\end{frame}

\begin{frame}
\frametitle{The task}
\begin{center}
\includegraphics[scale=0.4]{img/illustration_traffic_light.png}
\end{center}
\end{frame}

\begin{frame}{Presentation}
\begin{block}{}
\begin{itemize}
\item Part I: theoretical insight 
\item Part II: results from first implementations
\end{itemize}
\end{block}
\end{frame}

\begin{frame}
\frametitle{Reinforcement learning}
\centering
\includegraphics[scale=0.5]{img/RL_graph.png}
\vspace{1cm}

The agent's job is to find a behavior that maximizes the long-run sum of values of the rewards.
\end{frame}

\begin{frame}
\frametitle{Markov Decision Process intuition}

\begin{figure}[ht]
\centering
    \begin{tikzpicture}[font=\sffamily]
    % Add the states
        \node[state,
              text=yellow,
              draw=none,
              fill=gray!50!black] (0) {0};
        \node[state,
              right=2cm of 0,
              text=yellow,
              draw=none, 
              fill=gray!50!black] (1) {1};
        \node[state,
              below=2cm of 1,
              text=yellow,
              draw=none, 
              fill=gray!50!black] (2) {2};
        \node[state,
        	  below = 2cm of 0,
        	  text=yellow,
        	  draw=none,
        	  fill=gray!50!black] (3) {3};
     

              
 \draw[every loop,
        auto=right,
        >=latex,
        draw=gray,
        fill=gray]
        
        	(0) edge[loop left, draw = green] node {$0.5$}(0)
        	(0) edge[ auto = right,draw = red]  node {$0.5$}(2)
        	(0) edge[auto = right,draw = red] node{$0.5$}  (3)
            (0) edge[auto = left,draw=green]  node {$0.5$} (1)
            (1) edge[bend left,auto =left]  node {$1$} (0)
            (2) edge[]  node {$0.5$}(1)
			(2) edge[loop right] node{$0.5$} (2)
			(3) edge[loop right] node{$1$} (3)	
			       
            ;       
      
    \end{tikzpicture}

\end{figure}
Sequence of events : $s_0,a_1,s_1,r_1,a_2,\ldots$

\end{frame}






\begin{frame}
\frametitle{Markov Decision Process}
\begin{block}{Definition: (MDP) }
\begin{itemize}
\item A set of states $\mathcal{S}=\{s_0,s_1,s_2,\ldots,s_{n}\}$
\item A set of actions $\mathcal{A}=\{a_1,a_2,a_3,\ldots,a_{k}\}$
\item A transition function $T(a,s,s') = \mathbb{P}[s'\mid a,s]$
\item A reward function $R: \mathcal{S}\mapsto \mathbb{R}$
\item A discount factor $0 \leq \gamma < 1$ 
\end{itemize}
\end{block}

\begin{block}{Markov property}
The transitions only depends on the current state and the current action.
\end{block}
\end{frame}

%\begin{frame}
%\frametitle{to be told}
%Particularly, at each time step, our process is in some state $s$.\\Then our learning agent decides which action to execute from the set $\mathcal{A}$ which is doable from state $s$.\\Then the process moves randomly to a new state $s'$ following $T$ and gives the agent a reward $R(s')$.\\The purpose of our agent is to maximise the cumulative reward it gets in the long run.
%
%\end{frame}





\begin{frame}
\frametitle{How to pick actions}
\begin{block}{Definition: (policy)}
A \emph{policy} $\pi$ is a probabilistic mapping from the set of states to the set of actions : 
$$ \pi : \mathcal{A} x \mathcal{S} \mapsto [0,1] $$
\\
s.t. $\underset{a}{\sum } \pi(a\mid s)=1 \quad \forall s \in \mathcal{S}$

\end{block}
\end{frame}

\begin{frame}
\frametitle{Issue}
\begin{alertblock}{How to}
How to asses the quality of policies so we can find the best one? 

What is the best policy?
\end{alertblock}
\end{frame}

\begin{frame}
\frametitle{How to asses the quality of policies}
\begin{block}{We are interested in maximizing the discounted return: $G_t$}
$$ G_t= \sum_{k=0}^{\infty}\gamma^k R_{t+k+1} = R_{t+1} + \gamma G_{t+1}$$
\end{block}

\pause
\begin{block}{Action value while in a state s under $\pi$}
\begin{equation*}
q_{\pi}(s,a) = \mathbb{E}_{\pi}[G_t\mid S_t =s,A_t=a]
\end{equation*}
\end{block}

\pause
\begin{block}{State value under policy $\pi$}
\begin{equation*}
v_{\pi}(s)=\mathbb{E}_{\pi}[G_t \mid S_t=s]
\end{equation*}
\end{block}



\end{frame}

\begin{frame}
\frametitle{Recursive definition of value functions}
\begin{block}{action value}
\begin{equation*}
\begin{split}
q_{\pi}(s,a) &= \mathbb{E}_{\pi}[G_t\mid S_t =s, A_t=a]
\\&=\mathbb{E}_{\pi}[R_{t+1}+\gamma G_{t+1}\mid S_t =s, A_t=a]
\\& =\sum_{s'}\mathbb{P}(s'\mid a,s)\left[r + \gamma \mathbb{E}_{\pi}\left[G_{t+1} \mid S_{t+1}=s' \right]\right]
\\& = \sum_{s'}\mathbb{P}(s'\mid a,s)\left[r + \gamma v_{\pi}(s') \right]
\end{split}
\end{equation*}
\end{block}

with $r=R(s')$
\end{frame}


\begin{frame}
\frametitle{Recursive definition of value functions}
\begin{block}{state value}
\begin{equation*}
\begin{split}
v_{\pi}(s) &= \mathbb{E}_{\pi}[G_t\mid S_t =s]
\\&=\mathbb{E}_{\pi}[R_{t+1}+\gamma G_{t+1}\mid S_t =s]
\\& =\sum_{a }\pi(a \mid s) \sum_{s'}\mathbb{P}(s'\mid a,s)\left[r + \gamma \mathbb{E}_{\pi}\left[G_{t+1} \mid S_{t+1}=s' \right]\right]
\\& =\sum_{a }\pi(a \mid s) \sum_{s'}\mathbb{P}(s'\mid a,s)\left[r + \gamma v_{\pi}(s') \right]
\\&=\sum_{a }\pi(a \mid s)q_{\pi}(s,a)
\end{split}
\end{equation*}
\end{block}

with $r=R(s')$
\end{frame}




\begin{frame}
\frametitle{How to asses the quality of policies}
\begin{block}{How to compare two policies}
$$ \pi \leq \pi ' \iff  v_{\pi}(s) \leq v_{\pi'}(s) \quad \forall s \in \mathcal{S}
$$
\end{block}

\pause 

\begin{block}{Optimal policy}
$$ \pi_{*} \quad s.t. \quad \forall \pi : \pi_{*}\geq \pi $$
\end{block}
\end{frame}

%\begin{frame}
%\frametitle{Bellman optimality principle}
% An optimal policy has the property that whatever the initial state and initial decision are, the remaining decisions must constitute an optimal policy with regard to the state resulting from the first decision. (See Bellman, 1957, Chap. III.3.)
%\end{frame}

\begin{frame}
\frametitle{Bellman optimality equations}

The optimal policy $\pi_{*}$ has value functions  $v_*$ and $q_*$

\begin{block}{}

\begin{equation*}
\begin{split}
v_{*}(s)&=\underset{a}{\text{max }} q_{*}(s,a)
\\&=\max_{a} \sum_{s'}\mathbb{P}(s'\mid s,a)[R(s')+\gamma v_{*}(s')]
\end{split}
\end{equation*}
\end{block}

\begin{block}{}
\begin{equation*}
\begin{split}
q_{*}(s,a)&=\mathbb{E}\left[ R_{t+1} + \gamma v_{*}(S_ {t+1}) \mid S_t = s, A_t = a \right]
\\&=\mathbb{E}\left[ R_{t+1} + \gamma \max_{a'} q_{*}(S_{t+1},a') \mid S_t = s, A_t = a \right]
\\&= \sum_{s'}\mathbb{P}(s' \mid s,a)[R(s')+\gamma \max_{a'}q_{*}(s',a')]
\end{split}
\end{equation*}
\end{block}

%Intuitively these equations say that the value of a state under the optimal policy must equal the expected return for the best action from that state. For finite MDPs these equations have a unique solution.
\end{frame}


\begin{frame}
\frametitle{Finding optimal policy from value functions}
\begin{block}{}

$$\pi_{*}(s)= \underset{a}{\text{argmax }}q_*(s,a)$$
\end{block}
\end{frame}

\begin{frame}
\frametitle{Another issue}
\begin{alertblock}{Computational issue}
$\mid \mathcal{S} \mid $ linear equations to solve to evaluate policy\\
%\begin{center}
% $O(\mid \mathcal{S} \mid ^2)$
% \end{center}
$\mid \mathcal{S} \mid $ non linear equations to solve the Bellman optimality equation 
%\begin{center}
%$O(\mid \mathcal{S} \mid ^3)$ blablablabla
%\end{center}
\end{alertblock}

\pause 
\vspace{1cm}
\centering
Approximation of value function
\\ and
\\Policy iteration

\end{frame}

\begin{frame}
\frametitle{Solving MDP using dynamic programming}
\begin{block}{iterative policy evaluation}
update rule : 
$$ v_{k+1}(s)=\sum_{a \in \mathcal{A}}\pi(a \mid s)\sum_{s'}T(a,s,s')\left[R(s')+\gamma v_k(s')\right] $$
\end{block}
\pause
\begin{block}{policy improvement}
$\pi/\pi'$ : old/new policy.
$$\pi'(s) = \underset{a \in \mathcal{A}}{\text{argmax } } q_{\pi}(s,a) $$
\end{block}

\end{frame}


\begin{frame}
\frametitle{Policy iteration algorithm}
%\includegraphics[scale=0.6]{img/generalized_policy_iter_sutton.png}
\centering
\includegraphics[scale=0.7]{img/policy_iter_sutton.PNG}
\footnote{From (Sutton \& Barto, 1998)}

\end{frame}


\begin{frame}
\frametitle{Our problem}
\begin{center}
\includegraphics[scale=0.4]{img/illustration_traffic_light.png}
\end{center}
\end{frame}


\begin{frame}
\frametitle{First a simpler problem}
\begin{center}
\includegraphics[scale=0.4]{img/illustration_lava.png}
\end{center}
\end{frame}

\begin{frame}
\frametitle{Modelization}
\begin{block}{states}
\begin{itemize}
\item  position \{0,1,2,\ldots ,L, Lava  \}
\item  speed    \{low, medium, high   \}
\end{itemize}
\end{block}
\pause
\begin{block}{actions}
\begin{itemize}
\item  decelerating 
\item  maintaining speed
\item  accelerating 
\end{itemize}
\end{block}
\pause
\begin{block}{reward function}
\begin{itemize}
\item Lava: reward of -L
\item L in low speed: reward of +L
\item any other state : reward of -1
\end{itemize}
\end{block}

\end{frame}


\begin{frame}
\begin{center}
\frametitle{Keeping the same speed graph}

\begin{tikzpicture}[font=\sffamily]
    % Add the states
        \node[state,
              text= red ,
              ] (h0) {0};
		\node[state,
			below=1 cm of h0,
        	text=orange,
        	] (m0) {0};
		\node[state,
			below= 1 cm of m0,
        	text=black,
        	] (l0) {0};
         \node[state,
         	 right = 1 cm of h0,
              text=red,
              ] (h1) {1};	
		\node[state,
			below=1 cm of h1,
        	text=orange,
        	] (m1) {1};
		\node[state,
			below= 1 cm of m1,
        	text=black,
        	] (l1) {1};
         \node[state,
         	right = 1 cm of h1,
              text=red,
              ] (h2) {2};	
		\node[state,
			below=1 cm of h2,
        	text=orange,
        	] (m2) {2};
		\node[state,
		below= 1 cm of m2,
        	text=black,
        	] (l2) {2};
        \node[state,
        	right = 1 cm of h2,
              text=red,
             ] (h3) {3};	
		\node[state,
			below=1 cm of h3,
        	text=orange,
        	] (m3) {3};
		\node[state,
			below= 1 cm of m3,
        	text=black,
        	] (l3) {3};        	       	
			
		        
       \draw[every loop,
        auto=right,
        >=latex,
        draw=gray,
        fill=gray]
			(l0) edge[](l1)
			(l1) edge[] (l2)	
			(l2) edge[] (l3)
			(m0) edge[bend left] (m2)
			(m1) edge[bend right] (m3)
			(h0) edge[bend left] (h3)
	       
            ;
    \end{tikzpicture}
     \end{center}

 red : high speed \\ orange : medium speed \\black : low speed

\end{frame}

\begin{frame}
\begin{center}
\frametitle{Accelerating graph}
\begin{tikzpicture}[font=\sffamily]
    % Add the states
        \node[state,
              text= red ,
              ] (h0) {0};
		\node[state,
			below=1 cm of h0,
        	text=orange,
        	] (m0) {0};
		\node[state,
			below= 1 cm of m0,
        	text=black,
        	] (l0) {0};
         \node[state,
         	 right = 1 cm of h0,
              text=red,
              ] (h1) {1};	
		\node[state,
			below=1 cm of h1,
        	text=orange,
        	] (m1) {1};
		\node[state,
			below= 1 cm of m1,
        	text=black,
        	] (l1) {1};
         \node[state,
         	right = 1 cm of h1,
              text=red,
              ] (h2) {2};	
		\node[state,
			below=1 cm of h2,
        	text=orange,
        	] (m2) {2};
		\node[state,
		below= 1 cm of m2,
        	text=black,
        	] (l2) {2};
        \node[state,
        	right = 1 cm of h2,
              text=red,
             ] (h3) {3};	
		\node[state,
			below=1 cm of h3,
        	text=orange,
        	] (m3) {3};
		\node[state,
			below= 1 cm of m3,
        	text=black,
        	] (l3) {3};            	       	
			
		        
       \draw[every loop,
        auto=right,
        >=latex,
        draw=gray,
        fill=gray]
			(l0) edge[](m2)
			(l1) edge[] (m3)	
			(m0) edge[] (h3)
			(h0) edge[bend left] (h3)
	       
            ;
    \end{tikzpicture}
 \end{center}
\end{frame}


\begin{frame}
\begin{center}
\frametitle{Decelerating}
\begin{tikzpicture}[font=\sffamily]
    % Add the states
        \node[state,
              text= red ,
              ] (h0) {0};
		\node[state,
			below=1 cm of h0,
        	text=orange,
        	] (m0) {0};
		\node[state,
			below= 1 cm of m0,
        	text=black,
        	] (l0) {0};
         \node[state,
         	 right = 1 cm of h0,
              text=red,
              ] (h1) {1};	
		\node[state,
			below=1 cm of h1,
        	text=orange,
        	] (m1) {1};
		\node[state,
			below= 1 cm of m1,
        	text=black,
        	] (l1) {1};
         \node[state,
         	right = 1 cm of h1,
              text=red,
              ] (h2) {2};	
		\node[state,
			below=1 cm of h2,
        	text=orange,
        	] (m2) {2};
		\node[state,
		below= 1 cm of m2,
        	text=black,
        	] (l2) {2};
        \node[state,
        	right = 1 cm of h2,
              text=red,
             ] (h3) {3};	
		\node[state,
			below=1 cm of h3,
        	text=orange,
        	] (m3) {3};
		\node[state,
			below= 1 cm of m3,
        	text=black,
        	] (l3) {3};            	       	
			
		        
       \draw[every loop,
        auto=right,
        >=latex,
        draw=gray,
        fill=gray]
			(l0) edge[loop below](l0)
			(l1) edge[loop below] (l1)	
			(l2) edge[loop below] (l2)
			(l3) edge[loop below] (l3)
			(m0) edge[] (l1)
			(m1) edge[] (l2)
			(m2) edge[] (l3)
			(h0) edge[] (m2)
			(h1) edge[] (m3)
	       
            ;
    \end{tikzpicture}
 \end{center}
\end{frame}


\begin{frame}
\frametitle{State value function at different iterations}
\framesubtitle{the brighter the better the value}

\centering
\includegraphics[scale=0.12]{img/value0.jpg}\\
\begin{block}{}
\centering
States values at iteration $0$ 
\end{block}

\end{frame}




\begin{frame}
\frametitle{State value function at different iterations}
\framesubtitle{the brighter the better the value}
\centering
\includegraphics[scale=0.12]{img/value2.jpg}\\
\begin{block}{}
\centering
States values at iteration $2$ 
\end{block}
\end{frame}


\begin{frame}
\frametitle{State value function at different iterations}
\framesubtitle{the brighter the better the value}

\centering
\includegraphics[scale=0.12]{img/value4.jpg}\\
\begin{block}{}
\centering
States values at iteration $4$ 
\end{block}
\end{frame}

\begin{frame}
\frametitle{State value function at different iterations}
\framesubtitle{the brighter the better the value}

\centering
\includegraphics[scale=0.12]{img/value6.jpg}\\
\begin{block}{}
\centering
States values at iteration $6$ (where stable policy is attained)
\end{block}

\end{frame}



\begin{frame}
\frametitle{Results for deterministic actions}
\framesubtitle{One of the optimal trajectories}
\centering
\includegraphics[scale=0.5]{img/trajectory.jpg}
\end{frame}

\begin{frame}
\frametitle{Results for stochastic actions}
\framesubtitle{decelerating  results in keeping the same speed with probability 1/4}
\centering
\includegraphics[scale=0.5]{img/trajectory1.jpg}
\end{frame}



\begin{frame}
\frametitle{Results for stochastic actions}
\framesubtitle{decelerating results results in keeping the same speed with probability 1/4}
\centering
\includegraphics[scale=0.5]{img/trajectory4.jpg}\\
\centering
where stable policy is attained 
\end{frame}


\begin{frame}
\frametitle{Results for stochastic actions}
\framesubtitle{state-value function at the end of the iterations
}
\centering
\includegraphics[scale=0.7]{img/state_values_sto.jpg}
\end{frame}


\begin{frame}
\frametitle{Conclusion }
\begin{block}{}
\begin{itemize}
\item We introduced the theoretical framework: Reinforcement learning and Markov Decision Processes
\item We solved a simple problem where we know the model (the transition function in particular)
\end{itemize}
\end{block}
\end{frame}



\begin{frame}
\frametitle{What's next ? }



\begin{block}{already working on}
\begin{itemize}
\item Add the traffic light into this setting
\item Find the distance from the robot's camera to the traffic light
\end{itemize}
\end{block}

\begin{block}{In a not so distant future}
\begin{itemize}
\item Explore other algorithm and compare them : Monte-Carlo methods, temporal difference learning, Q learning, $\ldots$
\item Neuro-dynamic programming ?
\end{itemize}
\end{block}

\pause
\begin{block}{Ultimately}
Implement everything on the robot ... \pause and pray that everything works well
\end{block}

\end{frame}




\end{document}
