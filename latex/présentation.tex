\documentclass[dvipsnames,svgnames]{beamer}
\usetheme{Warsaw}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{tikz}
\usetikzlibrary{automata, positioning}
\usepackage{float}
\usepackage[normalem]{ulem}




\author{Charles Dufour}
\title{Reinforcement learning and robot navigation using MDPs}
%\setbeamercovered{transparent} 
\setbeamertemplate{navigation symbols}{} 
%\logo{} 
%\institute{} 
%\date{} 
%\subject{} 

\newtheorem{madef}{Definition}

\begin{document}


\begin{frame}
\titlepage
\end{frame}

%\begin{frame}
%\tableofcontents
%\end{frame}



\begin{frame}
\frametitle{Introduction}
\begin{block}{The problem}

  \begin{itemize}
   \item Framework : the Disopt robot which can follow lines
   \item The problem : the robot should adapt its speed with           respect to traffic lights
   \item How : using Markov Decision Process (MDP) and Reinforcement Learning (RL)
  \end{itemize}
\end{block} 
\end{frame}


%\begin{frame}
%\frametitle{MDPs}
%\begin{madef}
%A Markov Decision Process (MDP) is a discrete time stochastic control process, used in situations where outcomes are and random and partly under the control of a decision maker. 
%
%\end{madef}
%\end{frame}

\begin{frame}
\frametitle{MDPs}
\begin{block}{Definition }
\begin{itemize}
\item A set of states $\mathcal{S}=\{s_0,s_1,s_2,\ldots\}$
\item A set of actions $\mathcal{A}=\{a_1,a_2,a_3,\ldots\}$
\item A transition function $T(a,s,s',r) = \mathbb{P}[s',r\mid a,s]$
\item A reward function $R: \mathcal{S}\mapsto \mathbb{R}$
\item A discount factor $\gamma$ 
\end{itemize}
\end{block}

\begin{block}{Markov Property}
The transitions only depends on the current state and the current action.
\end{block}
\end{frame}

%\begin{frame}
%\frametitle{to be told}
%Particularly, at each time step, our process is in some state $s$.\\Then our learning agent decides which action to execute from the set $\mathcal{A}$ which is doable from state $s$.\\Then the process moves randomly to a new state $s'$ following $T$ and gives the agent a reward $R(s')$.\\The purpose of our agent is to maximise the cumulative reward it gets in the long run.
%
%\end{frame}


\begin{frame}
\frametitle{MDP example}
MDPs can easily be represented by graphs :
\begin{figure}[ht]
\centering
    \begin{tikzpicture}[font=\sffamily]
    % Add the states
        \node[state,
              text=yellow,
              draw=none,
              fill=gray!50!black] (0) {0};
        \node[state,
              right=2cm of 0,
              text=yellow,
              draw=none, 
              fill=gray!50!black] (1) {1};
        \node[state,
              right=2cm of 1,
              text=yellow,
              draw=none, 
              fill=gray!50!black] (2) {2};
         \node[state,
              right=2cm of 2,
              text=yellow,
              draw=none, 
              fill=gray!50!black] (3) {3};         

              
       \draw[every loop,
        auto=right,
        >=latex,
        draw=gray,
        fill=gray]
        
        	(0) edge[loop above] node {$p_{0,0}$}(0)
        	(0) edge[bend left, auto = left]  node {$p_{0,2}$}(2)
            (0) edge[auto = left]  node {$p_{0,1}$} (1)
            (1) edge[bend left,auto =left]  node {$p_{1,0}$} (0)
            (2) edge[]  node {$p_{2,1}$}(1)
			(2) edge[loop above] node{$p_{2,2}$} (2)
			(1) edge[bend right,  auto = right] node{$p_{1,3}$} (3)
			(3) edge[loop above] node{$p_{3,3}$} (3)           
            ;
            
              
    \end{tikzpicture}

\end{figure}

The constraints are $\sum_{j}p_{i,j}=1 \quad \forall i \in \mathcal{S}$

\end{frame}


\begin{frame}
\frametitle{how to pick actions}
\begin{madef}
A \emph{policy} $\pi$ is a probabilistic mapping from the set of states to the set of actions : 
$$ \pi : \mathcal{S} \mapsto \mathcal{A} $$

\end{madef}
\end{frame}

\begin{frame}
\frametitle{Issue}
\begin{alertblock}{How to ?}
How to asses the goodness of policies so we can find the best one ? 

What is the best policy ?
\end{alertblock}
\end{frame}

\begin{frame}
\frametitle{how to asses the goodness of policies}
\begin{block}{Discounted return}
$$ G_t= \sum_{k=0}^{\infty}\gamma^k * R_{t+k+1}$$
\end{block}

\pause
\begin{block}{action value while in a state s under $\pi$}
\begin{equation}
q_{\pi}(s,a) = \mathbb{E}[G_t\mid S_t =s,A_t=a]
\end{equation}
\end{block}

\pause
\begin{block}{state value under policy $\pi$}
\begin{equation}
\begin{split}
v_{\pi}(s)&=\mathbb{E}[G_t \mid S_t=s] 
\\& = \sum_{a}\pi(a\mid s)\sum_{r,s'}p(s',r\mid s,a)[r+\gamma v_{\pi}(s')]
\end{split}
\end{equation}
\end{block}



\end{frame}

\begin{frame}
\frametitle{how to asses the goodness of policies}
\begin{block}{how to compare two policies}
$$ \pi \leq \pi ' \iff  \pi (s)\leq \pi ' (s) \quad \forall s \in \mathcal{S}
$$
\end{block}

\pause 

\begin{block}{Optimal policy}
$$ \pi_{*} \quad s.t. \quad \forall \pi : \pi_{*}\geq \pi $$
\end{block}
\end{frame}


\begin{frame}
\frametitle{Bellman optimality equations}

The optimal policy $\pi_{*}$ has value functions : $v_*$ and $q_*$

\begin{block}{}

\begin{equation}
v_{*}(s)=\max_{a} \sum_{s',r}p(s',r\mid s,a)[r+\gamma v_{*}(s')]
\label{bellman_opt_v}
\end{equation}

\begin{equation}
q_{*}(s,a)= \sum_{s',r}p(s',r \mid s,a)[r+\gamma \max_{a'}q_{*}(s',a')]
\label{bellman_opt_q}
\end{equation}
\end{block}

%Intuitively these equations say that the value of a state under the optimal policy must equal the expected return for the best action from that state. For finite MDPs these equations have a unique solution.
\end{frame}

\begin{frame}
\frametitle{Another issue}
\begin{alertblock}{computational issue}
If we wanted to solve these equations directly, it would cost a lot of computational power to know exactly the value functions first and then to solve. So how do we do it ? 
\end{alertblock}

\pause 
\vspace{1cm}
\centering
Approximation of value function

\end{frame}

\begin{frame}
\frametitle{solving MDPs using dynamic programming}
\begin{block}{policy iteration}
update rule : 
$$ v_{k+1}(s)=\sum_{a \in \mathcal{A}}\pi(a \mid s)\sum_{s',r}p(s',r\mid s,a)(r+\gamma v_k(s')) $$
\end{block}
\pause
\begin{block}{Policy Improvement}
$\pi/\pi'$ : old/new policy.
$$\pi'(s) = argmax_{a \in \mathcal{A}}q_{\pi}(s,a) $$
\end{block}

\end{frame}


\begin{frame}
\frametitle{what have we done so far}
\begin{center}
\includegraphics[scale=0.4]{img/illustration2.png}
\end{center}
\end{frame}

\begin{frame}
\frametitle{modelization}
\begin{block}{States}
\begin{itemize}
\item \pause position \{0,1,2,\ldots ,L, Lava  \}
\item \pause speed    \{low, medium, high   \}
\end{itemize}
\end{block}
\pause
\begin{block}{Actions}
\begin{itemize}
\item \pause decelerating 
\item \pause maintaining speed
\item \pause accelerating 
\end{itemize}
\end{block}
\end{frame}


\begin{frame}
\begin{center}
\frametitle{accelerating graph}
\begin{tikzpicture}[font=\sffamily]
    % Add the states
        \node[state,
              text=red,
              draw=none,
              fill=gray!50!black] (h0) {0};	
		\node[state,
			below=1 cm of h0,
        	text=yellow,
        	draw=none,
        	fill=gray!50!black] (m0) {0};
		\node[state,
			below= 1 cm of m0,
        	text=green,
        	draw=none,
        	fill=gray!50!black] (l0) {0};
         \node[state,
         	 right = 1 cm of h0,
              text=red,
              draw=none,
              fill=gray!50!black] (h1) {1};	
		\node[state,
			below=1 cm of h1,
        	text=yellow,
        	draw=none,
        	fill=gray!50!black] (m1) {1};
		\node[state,
			below= 1 cm of m1,
        	text=green,
        	draw=none,
        	fill=gray!50!black] (l1) {1};
         \node[state,
         	right = 1 cm of h1,
              text=red,
              draw=none,
              fill=gray!50!black] (h2) {2};	
		\node[state,
			below=1 cm of h2,
        	text=yellow,
        	draw=none,
        	fill=gray!50!black] (m2) {2};
		\node[state,
		below= 1 cm of m2,
        	text=green,
        	draw=none,
        	fill=gray!50!black] (l2) {2};
        \node[state,
        	right = 1 cm of h2,
              text=red,
              draw=none,
              fill=gray!50!black] (h3) {3};	
		\node[state,
			below=1 cm of h3,
        	text=yellow,
        	draw=none,
        	fill=gray!50!black] (m3) {3};
		\node[state,
			below= 1 cm of m3,
        	text=green,
        	draw=none,
        	fill=gray!50!black] (l3) {3};        	       	
			
		        
       \draw[every loop,
        auto=right,
        >=latex,
        draw=gray,
        fill=gray]
			(l0) edge[](l1)
			(l1) edge[] (l2)	
			(l2) edge[] (l3)
			(m0) edge[bend left] (m2)
			(m1) edge[bend right] (m3)
			(h0) edge[bend left] (h3)
	       
            ;
    \end{tikzpicture}
     \end{center}

 red : high speed \\ yellow : medium speed \\green : low speed

\end{frame}

\begin{frame}
\begin{center}
\frametitle{keeping the same speed graph}
\begin{tikzpicture}[font=\sffamily]
    % Add the states
        \node[state,
              text=red,
              draw=none,
              fill=gray!50!black] (h0) {0};	
		\node[state,
			below=1 cm of h0,
        	text=yellow,
        	draw=none,
        	fill=gray!50!black] (m0) {0};
		\node[state,
			below= 1 cm of m0,
        	text=green,
        	draw=none,
        	fill=gray!50!black] (l0) {0};
         \node[state,
         	 right = 1 cm of h0,
              text=red,
              draw=none,
              fill=gray!50!black] (h1) {1};	
		\node[state,
			below=1 cm of h1,
        	text=yellow,
        	draw=none,
        	fill=gray!50!black] (m1) {1};
		\node[state,
			below= 1 cm of m1,
        	text=green,
        	draw=none,
        	fill=gray!50!black] (l1) {1};
         \node[state,
         	right = 1 cm of h1,
              text=red,
              draw=none,
              fill=gray!50!black] (h2) {2};	
		\node[state,
			below=1 cm of h2,
        	text=yellow,
        	draw=none,
        	fill=gray!50!black] (m2) {2};
		\node[state,
		below= 1 cm of m2,
        	text=green,
        	draw=none,
        	fill=gray!50!black] (l2) {2};
        \node[state,
        	right = 1 cm of h2,
              text=red,
              draw=none,
              fill=gray!50!black] (h3) {3};	
		\node[state,
			below=1 cm of h3,
        	text=yellow,
        	draw=none,
        	fill=gray!50!black] (m3) {3};
		\node[state,
			below= 1 cm of m3,
        	text=green,
        	draw=none,
        	fill=gray!50!black] (l3) {3};        	       	
			
		        
       \draw[every loop,
        auto=right,
        >=latex,
        draw=gray,
        fill=gray]
			(l0) edge[](m2)
			(l1) edge[] (m3)	
			(m0) edge[] (h3)
			(h0) edge[bend left] (h3)
	       
            ;
    \end{tikzpicture}
 \end{center}
\end{frame}


\begin{frame}
\begin{center}
\frametitle{decelerating}
\begin{tikzpicture}[font=\sffamily]
    % Add the states
        \node[state,
              text=red,
              draw=none,
              fill=gray!50!black] (h0) {0};	
		\node[state,
			below=1 cm of h0,
        	text=yellow,
        	draw=none,
        	fill=gray!50!black] (m0) {0};
		\node[state,
			below= 1 cm of m0,
        	text=green,
        	draw=none,
        	fill=gray!50!black] (l0) {0};
         \node[state,
         	 right = 1 cm of h0,
              text=red,
              draw=none,
              fill=gray!50!black] (h1) {1};	
		\node[state,
			below=1 cm of h1,
        	text=yellow,
        	draw=none,
        	fill=gray!50!black] (m1) {1};
		\node[state,
			below= 1 cm of m1,
        	text=green,
        	draw=none,
        	fill=gray!50!black] (l1) {1};
         \node[state,
         	right = 1 cm of h1,
              text=red,
              draw=none,
              fill=gray!50!black] (h2) {2};	
		\node[state,
			below=1 cm of h2,
        	text=yellow,
        	draw=none,
        	fill=gray!50!black] (m2) {2};
		\node[state,
		below= 1 cm of m2,
        	text=green,
        	draw=none,
        	fill=gray!50!black] (l2) {2};
        \node[state,
        	right = 1 cm of h2,
              text=red,
              draw=none,
              fill=gray!50!black] (h3) {3};	
		\node[state,
			below=1 cm of h3,
        	text=yellow,
        	draw=none,
        	fill=gray!50!black] (m3) {3};
		\node[state,
			below= 1 cm of m3,
        	text=green,
        	draw=none,
        	fill=gray!50!black] (l3) {3};        	       	
			
		        
       \draw[every loop,
        auto=right,
        >=latex,
        draw=gray,
        fill=gray]
			(l0) edge[loop below](l0)
			(l1) edge[loop below] (l1)	
			(l2) edge[loop below] (l2)
			(l3) edge[loop below] (l3)
			(m0) edge[] (l1)
			(m1) edge[] (l2)
			(m2) edge[] (l3)
			(h0) edge[] (m2)
			(h1) edge[] (m3)
	       
            ;
    \end{tikzpicture}
 \end{center}
\end{frame}


\begin{frame}
\frametitle{Results}
\framesubtitle{States value evolution}
\begin{block}{}
\centering
States values at iterations $0,2,4$ and $6$ (where stable policy is attained)
\end{block}
\vspace{1cm}
\centering
\includegraphics[scale=0.1]{img/value0.jpg}\\
\includegraphics[scale=0.1]{img/value2.jpg}\\
\includegraphics[scale=0.1]{img/value4.jpg}\\
\includegraphics[scale=0.1]{img/value6.jpg}\\




\end{frame}
\begin{frame}
\frametitle{Results}
\framesubtitle{Discounted reward}
\centering
\includegraphics[scale=0.1]{img/scores.jpg}
%\includegraphics[scale=0.05]{img/scores_rand.jpg}
\end{frame}


\begin{frame}
\frametitle{What's next ? }
\begin{block}{already working on}
\begin{itemize}
\item Add the traffic light into this setting
\end{itemize}
\end{block}

\begin{block}{In a not so distant future}
\begin{itemize}
\item Finding the distance from the robot's camera to the traffic light
\item implement on the robot... \pause and pray that everything works well on the first try
\end{itemize}

\end{block}
\end{frame}



\end{document}
