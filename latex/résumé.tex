\documentclass[14pt,a4paper]{article}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{makeidx}
\usepackage{amsthm}
\usepackage[margin=48pt]{geometry}
\usepackage[linesnumbered,ruled]{algorithm2e}
\usepackage{tikz}
\usetikzlibrary{automata, positioning}
\usepackage{float}
\usepackage[round]{natbib}
\usepackage[hidelinks]{hyperref}
\usepackage{caption}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{listings}
\usepackage{color}
%\usepackage[justification=centering]{caption}


\def\code#1{\texttt{#1}}
\newtheorem{theorem}{Theorem}
\def\iff{\Leftrightarrow}
\theoremstyle{definition}
\newtheorem{madef}{Definition}
\newtheorem{prop}{Proposition}


 
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=left,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2
}
 
\lstset{style=mystyle}



\oddsidemargin  0in
\evensidemargin  0in
\textwidth   6.3in
\textheight  9.5in
\topmargin  -0.7in


\usepackage{fancyhdr}
\pagestyle{fancy}
%\lfoot{Charles DUFOUR}
%\rfoot{\today}
\fancyhead[R]{}


\author{Charles Dufour}
\title{Bachelor Project : \\
Reinforcement learning and robot navigation}
\begin{document}

\begin{titlepage}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page
 
%----------------------------------------------------------------------------------------
%   HEADING SECTIONS
%----------------------------------------------------------------------------------------

\vspace{3cm}
\textsc{\LARGE \'Ecole polytechnique f\'ed\'erale de Lausanne}\\[0.5cm] % Name of your university/college
\textsc{\large Disopt}\\[1.5cm] % Name of your university/college
\textsc{\LARGE Bachelor project}\\[0.5cm] % Major heading such as course name
\textsc{\large Spring Semester 2018 }\\[0.5cm] % Minor heading such as course title

%----------------------------------------------------------------------------------------
%   TITLE SECTION
%----------------------------------------------------------------------------------------

\HRule \\[0.4cm]
{ \huge \bfseries Reinforcement learning and robot navigation}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]
 
%----------------------------------------------------------------------------------------
%   AUTHOR SECTION
%----------------------------------------------------------------------------------------

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Student:}\\
Charles Dufour
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Supervisors:} \\
Prof. Friedrich Eisenbrand \\% Supervisor's Name
Jonas Racine
\end{flushright}
\end{minipage}\\[5cm]

%----------------------------------------------------------------------------------------
%   LOGO SECTION
%----------------------------------------------------------------------------------------

 \includegraphics[width=0.5\linewidth]{img/logo_EPFL.eps}\\[1cm] % Include a department/university logo - this will require the graphicx package
 
%----------------------------------------------------------------------------------------

\vfill % Fill the rest of the page with whitespace

\end{titlepage}



%\begin{titlepage}
%   \vspace*{\stretch{1.0}}
%   \begin{center}
%      \Large\textbf{Reinforcement learning and robot navigation}\\
%      
%      \large\textbf{ }\\
%      \large\textit{Bachelor Project}
%   \end{center}
%   \vspace*{\stretch{2.0}}
%   
%   \vfill 
%   {\centering Charles Dufour, EPFL\par}
%\end{titlepage}

\newpage


\begin{abstract}

This dissertation focuses on the intelligent robot control in autonomous navigation using reinforcement learning. More precisely, the purpose of this bachelor project is to make a raspberry pie 3 powered robot adapt its behavior based on traffic light using reinforcement learning, and more precisely Q-learning.We will first discuss about the theory, then we will implement the Q-learning on our robot, consider the results and then discuss about further work. This project is the successor of many projects on the robot in  DISOPT. 

\nocite{Xia2015}
\nocite{KLMSurvey}
\end{abstract}


\tableofcontents
\pagenumbering{gobble}
\newpage
\fancyhead[L]{\emph{THEORY}}


%\emph{Reinforcement learning}\\
%Reinforcement learning is learning what to do, how to map situations to actions so as to maximize
%a numerical reward signal. The learner is not told which actions to take, but instead must discover
%which actions yield the most reward by trying them. \citep{Sutton}
%
%Some technical terms : 
%\begin{itemize}
%\item \emph{policy} : it is a mapping from the states to the actions
%\item \emph{reward} : what our learning agent is trying to maximize
%\item \emph{model} of the environment : the laws governing the environment 
%\end{itemize}
%
%"Reinforcement learning methods specify how the agent's policy is changed as a result of its experience"\citep{Sutton}
%
%
%
%The usual way to formulate the reinforcement learning problem from a mathematical point of view is by using what we call Markov's decision processes (MDPs).




\pagenumbering{arabic}
\part*{Theory}
\section{Reinforcement learning, a short introduction}

\begin{figure}[H]
\centering
\includegraphics[scale=0.5]{img/RL_graph.png}
\caption{Graphical representation of the reinforcement learning process from \citep{Sutton}}
\label{RL}
\end{figure}

In reinforcement learning problems, a scalar value, which we will call the reward, is received by an agent for transitions to one state to another. The aim of the agent is to maximize the discounted sum of the rewards he will receive, and to do so has to find an appropriate behavior (policy).


Our agent is connected to the model trough perceptions and actions. The typical procedure is as follows: 
\begin{itemize}
\item the environment sends an information to the agent about the state he is actually into
\item then the agent chooses to do an action, based on the information he just received, the action takes him to another state in the environment
\item the environment sends back to the agent a reward (numerical signal, a single number) for the transition that just happened and also an information about the state the action brought our agent into
\item and so on $\ldots$
\end{itemize}

The purpose of our agent is to learn an optimal behavior which should optimize the long-run sum of values of the reinforcement signal (the rewards). \citep{KLMSurvey}

We define an \emph{episode} as being a finite sequence of event perceived by our agent (perception,action,reward,\ldots) The last state of an episode, the one that defines when it ends is called an \emph{absorbing state}.

\section{MDP : Markov decision processes} \label{def_MDP}



The mathematical way to define the reinforcement learning framework is done using Markov decision processes.

Markov's decision processes are a tuple  $\mathcal{M}=\{\mathcal{S},\mathcal{A},T,R,\gamma\}$ : 

\begin{itemize}
\item a set of states : $\mathcal{S}=\{s_0,s_1,\ldots,s_n\}$
\item a set of actions  : $\mathcal{A}=\{a_1,a_2,\ldots ,a_k\}$
\item a transition function :  $T(s,a,s') \sim  Pr(s'\mid a, s) \quad s,s' \in \mathcal{S}$ which gives the state transition probabilities
\item a reward function : $R:\mathcal{S}\mapsto \mathbb{R} $
\item a discount factor $\gamma \in [0,1)$
\end{itemize}


The discount factor $\gamma $ helps making our learning agent more or less far-sighted : $\gamma$ can be interpreted as the relative importance it will give to future rewards compared to immediate ones.

$\mathcal{M}$ is a MDP if it respects the Markovian property which is that the transitions only depends on the current state and action : 
\begin{equation*}
\mathbb{P}(s_{n+1}\mid a_{n+1},s_{n},a_{n},s_{n-1},\ldots)=\mathbb{P}(s_{n+1}\mid a_{n+1},s_{n})\\
\end{equation*}


When an agent is learning in a MDP, what it observes is a sequence of states, actions and rewards: suppose the agent is in the state $s_0$ and chooses action $a_1$ and then end up in state $s_1$ with reward $r_1$; then the sequence observed is of the form : $s_0,a_1,s_1,r_1,a_2,s_2,r_2\ldots$


Markov decisions processes can easily be  represented as a directed graph : the nodes are the states and the directed edges from a node $Q$ are the actions an agent can choose to make while being in state $Q$, which will bring the agent in the state represented by the node at the endpoint of the edge as we can see in figure \ref{figure : graph_example}.


\begin{figure}[H]
\centering
\captionsetup{justification=centering,margin=2cm}
\begin{tikzpicture}[font=\sffamily]
    % Add the states
        \node[state,
              text=yellow,
              draw=none,
              fill=gray!50!black] (0) {0};
        \node[state,
              right=2cm of 0,
              text=yellow,
              draw=none, 
              fill=gray!50!black] (1) {1};
        \node[state,
              below=2cm of 1,
              text=yellow,
              draw=none, 
              fill=gray!50!black] (2) {2};
        \node[state,
        	  below = 2cm of 0,
        	  text=yellow,
        	  draw=none,
        	  fill=gray!50!black] (3) {3};
     

              
 \draw[every loop,
        auto=right,
        >=latex,
        draw=gray,
        fill=gray]
        
        	(0) edge[loop left, draw = green] node {$0.5$}(0)
        	(0) edge[ auto = right,draw = red]  node {$0.5$}(2)
        	(0) edge[auto = right,draw = red] node{$0.5$}  (3)
            (0) edge[auto = left,draw=green]  node {$0.5$} (1)
            (1) edge[bend left,auto =left]  node {$1$} (0)
            (2) edge[]  node {$0.5$}(1)
			(2) edge[loop right] node{$0.5$} (2)
			(3) edge[loop right] node{$1$} (3)	
			       
            ;       
      
    \end{tikzpicture}
    \caption{example of a graph representing a Markov Decision process, there are two actions possible from node $0$ which takes to different states with some probability }
     \label{figure : graph_example}
   
\end{figure}  




\section{Policies and Value functions}

A policy : $\pi $ is a stochastic mapping from states to action: $$ \pi : \mathcal{A} \times \mathcal{S} \mapsto [0,1] \quad \text{s.t. } \sum_{a}\pi(a\mid s)=1 \quad \forall s \in \mathcal{S}$$
A policy is a formalization of the decision making process : in each state, we follow the policy to decide which action to choose with some probability: $\pi(a\mid s) $ is the probability that $A_{t+1}=a$ if $S_t=s$. Hence maximizing the reward means finding a good policy.


We need to quantify the reward the agent has to maximize, so we define the return as : 
$$G_t= r_{t+1}+\gamma r_{t+2}+ \dots = \sum_{k=0}^{\infty}\gamma^{k}r_{t+k+1} = r_{t+1} + \gamma G_{t+1} $$





Now we need a way to compare policy between them : for that we use the \emph{value functions} : the state value and the state-action value under a certain policy $\pi$. Intuitively they measure how good it is to be in a state (or respectively to take a specific action while being in a state) if we follow the policy afterwards.


Then we can define the value of taking action $a$ in state $s$ while following the policy $\pi$ as the expected return we would receive if we follow $\pi$:


\begin{equation*}
\begin{split}
v_{\pi}(s)&= \mathbb{E}_{\pi}[G_t \mid S_t=s]
\\&=\mathbb{E}_{\pi}\left[ r_{t+1} + \gamma G_{t+1} \mid S_t = s\right]
\\&=\mathbb{E}_{\pi}\left[ r_{t+1} \mid S_t = s \right] + \gamma \mathbb{E}_{\pi}\left[G_{t+1} \mid S_t = s \right]
\\&=\sum_{a}\pi(a \mid s)\sum_{s'}\mathbb{P}(s' \mid a,s)R(s') 
\\&+ \gamma \sum_{a}\pi(a \mid s)\sum_{s'}\mathbb{P}(s'\mid a,s)\mathbb{E}_{\pi}\left[G_{t+1} \mid S_{t+1} = s', S_{t}= s\right]
\end{split}
\end{equation*}

Using the Markovian property, $\mathbb{E}_{\pi}\left[G_{t+1} \mid S_{t+1}, S_{t}\right] = \mathbb{E}_{\pi}\left[G_{t+1} \mid S_{t+1}\right] $ so we get : 

\begin{equation}
\begin{split}
v_{\pi}(s)&= \sum_{a}\pi(a \mid s)\sum_{s'}\mathbb{P}(s'\mid a,s)\left[R(s') +\gamma \mathbb{E}_{\pi}\left[  G_{t+1} \mid S_{t+1}=s'  \right]\right]
\\&=\sum_{a}\pi(a \mid s)\sum_{s'}\mathbb{P}(s'\mid a,s)\left[R(s') + \gamma v_{\pi}(s')\right]
\end{split}
\label{v(s)}
\end{equation}



And the value of taking action $a$ while being in state $s$ under a policy $\pi$ following roughly the same method:

\begin{equation}
\begin{split}
q_{\pi}(s,a)&= \mathbb{E}\left[G_t\mid S_t=s, A_t=a\right]
\\&=\mathbb{E}\left[\sum_{k=0}^{\infty}\gamma^{k}r_{t+k+1}\mid S_t=s,A_t=a\right]
\\&= \sum_{s'}\mathbb{P}(s'\mid a,s)\left[R(s')+\gamma v_{\pi}(s')\right]
\end{split}
\label{q(s,a)}
\end{equation}



We can see from equations (\ref{v(s)}) and (\ref{q(s,a)})  that : 
\begin{equation}
v_{\pi}(s)=\sum_{a}\pi(a\mid s)q_{\pi}(s,a)
\end{equation}

These equations are called the Bellman equations and are used to find the value functions using dynamic programming, since they give recursive definition of the value functions.




\section{Optimal policies and Optimal value function}

For finite MDPs, the value function can define a partial order in the space of policies : 
$$ 
\pi\leq \pi' \iff v_{\pi} (s)\leq v_{\pi'} (s) \quad \forall s \in \mathcal{S}
$$

An optimal policy is a policy which is greater or equal than any other policy. This is what we are interested to find.

We can notice there is no policy strictly better than every deterministic policy \citep{Puterman} so there is always a deterministic optimal policy since there is only a finite number of deterministic policies.


We may have multiple optimal policies but they all have the same value functions otherwise they would not be optimal policies with respect to the partial order define earlier. We denote these functions $q_{*}$ and $v_{*}$.

\paragraph{Bellman optimality equations}

The optimal policy $\pi^*$ has  optimal value functions : $v_* $ and $ q_*$, which satisfy the relations below : 

\begin{equation}
\begin{split}
v_{*}(s)&=\max_{a}q_{*}(s,a)
\\&=\max_{a} \sum_{s'}\mathbb{P}(s'\mid a,s)\left[R(s')+\gamma v_{*}(s')\right]
\end{split}
\label{bellman_opt_v}
\end{equation}

\begin{equation}
\begin{split}
q_{*}(s,a)&=\mathbb{E}\left[ R_{t+1} + \gamma v_{*}(S_ {t+1}) \mid S_t = s, A_t = a \right]
\\&=\mathbb{E}\left[ R_{t+1} + \gamma \max_{a'} q_{*}(S_{t+1},a') \mid S_t = s, A_t = a \right]
\\&= \sum_{s'}\mathbb{P}(s' \mid s,a)[R(s')+\gamma \max_{a'}q_{*}(s',a')]
\end{split}
\label{bellman_opt_q}
\end{equation}

These are called the \emph{Bellman optimality equations}. What equation (\ref{bellman_opt_v}) tells intuitively is that the value of a state under $\pi^*$ must equal the expected return for the best action we can take from that state.


For finite MDPs these equations have a unique solution. We can note that if we know $v_{*}$ or $q_{*}$ a greedy approach to define a policy (best in the short term) becomes a long-term optimal solution : indeed defining a policy being greedy in function of $v_*$ implies that you go to the best state possible, the one with the biggest expected reward.


\section{Solving MDPs with dynamic programming}

In general we don't have all the information we need to compute the exact value of $v_{*}$ or even if we have them, we don't have the computational power needed. We often use approximation of value-function instead. In order to explore all the possible states, we choose the actions in an $\epsilon$-greedy manner: when we follow the policy $\pi$, with probability $\epsilon$ we will choose an action at random and with probability $1-\epsilon$ we will follow the policy. This allows us to find a solution of the exploration-exploitation problem (you need to improve the estimate you already have on the values, but also by exploring (trying new actions) you will find a better solution: see \citep{Sutton} chapter $2$ for more).


From now on we assume our MDPs are finite, even if it is possible to extend everything to infinite MDPs if we are careful enough to avoid the problematic ones. \citep{Sutton}

\subsection{Policy iteration}

This method uses two processes : the first one is policy evaluation : we compute the value function of a policy for all the states $s \in \mathcal{S} $; then we use policy improvement to get a better policy by acting greedy.
\paragraph{Policy evaluation} 


We begin by setting arbitrary $v(s) \quad \forall s \in \mathcal{S}$.
Then we update using one of the following update rules : 

\begin{itemize}
\item $v_{k+1}(s)=\underset{a \in A}{\sum}\pi(a \mid s)\underset{s'}{\sum}\mathbb{P}(s'\mid a,s)(R(s')+\gamma v_k(s'))$ \quad this is called "iterative policy evaluation".
\item we can use the same update rule as before, but use new information as soon as it is available : this kind of upgrade algorithm are called in places.
\end{itemize}

From this we derive the algorithm \ref{algo1}.


\begin{algorithm}

    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    %\underline{function Euclid} $(a,b)$\;
    \Input{policy to evaluate $\pi$}
    \Output{$V \approx v_{\pi}$}
    Initialize $V = 0$\\
    \While{$\Delta \geq \epsilon$}{
    	$\Delta = 0$\\
    	\For{$s\in \mathcal{S}$}{
    	 $v=V(s)$\\
    	 $V(s)=\underset{a}{\sum}\pi(a\mid s)\underset{s'}{\sum}\mathbb{P}(s'\mid a,s)[R(s')+\gamma V(s')]$\\
    	 $\Delta = \max(\Delta,\mid v-V(s)\mid)$\\
    	 }
       	}
       	\Return{$V \approx v_{\pi}$}
    
    \caption{Iterative policy evaluation (in place)}
    \label{algo1}
\end{algorithm}





\paragraph{Policy improvement}

Then we try to find a better policy : we try to determine whether we should change $\pi(s) $ to $ a\neq\pi(s)$. In order to do so, we try to first select action $a$ while being in state $s$ and then following the policy. If the expected reward we get by doing this choice is better than the one we get by simply following our policy, we should improve.

Mathematically speaking we will compare the value of taking action $a$ while being in the state s : $q_{\pi}(s,a)$ to the value of $s$: $v_{\pi}(s)$. Then we would greedily improve our policy this way.

The greedy update rule we use to improve our policy is the idea behind algorithm \ref{algo2} : 

$$ 
\pi'(s)=\text{arg}\max_{a \in A}q_{\pi}(s,a)
$$

\begin{algorithm}
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    %\underline{function Euclid} $(a,b)$\;
    \Input{policy to improve $\pi$}
    \Output{$\pi'$ s.t : $\pi' \geq \pi$ }
    \For{$s\in \mathcal{S}$}{
    	$\pi'(s)=\underset{a \in A}{\text{argmax }}q_{\pi}(s,a)$
    	}
       	\Return{$\pi'$}
    
    \caption{Policy improvement}
    \label{algo2}

\end{algorithm}



\paragraph{Policy Iteration} By combining the two processes described before, we can derive an algorithm to sweep through all our states and upgrade our policy until the changes between each sweep is too small : it is controlled by a parameter $\epsilon$ and a parameter $\gamma$ (which we talked about previously).This is the algorithm \ref{algo3} which can be represented graphically by Figure (\ref{fig:Sutton})



\begin{figure}[ht]
\centering
\captionsetup{justification=centering,margin=2cm}

\includegraphics[scale=0.7]{img/policy_iter_sutton.PNG}
\caption{Graphical representation of the interactions between policy improvement and policy evaluation(from \citep{Sutton})}
\label{fig:Sutton}
\end{figure}






Algorithm \ref{algo3} terminates since there is only a finite number of deterministic policies.
The reason why the algorithm~\ref{algo3} works is called the the \emph{policy improvement theorem}:

\begin{theorem}
Let $\pi$ and $\pi'$ be any pair of policies such that  $\forall s \in \mathcal{S} : $
\begin{equation}
q_{\pi}(s,\pi(s))\geq v_{\pi'}(s) \label{th1}
\end{equation}
then :
\begin{equation}
\pi \geq \pi' \label{th2}
\end{equation}


Moreover if there is a strict inequality in all the states in \ref{th1} then there must be at least a strict inequality for one state in \ref{th2}
\end{theorem}


\begin{proof}
The idea of the proof is to expand the $q_{\pi}$ side until we get $v_{\pi'}(s)$ using Equation~\ref{q(s,a)} in page \pageref{q(s,a)}.

Indeed we have : 
\begin{equation}
\begin{split}
v_{\pi}(s) &\leq q_{\pi}(s,\pi'(s))
\\&=\mathbb{E}[r_{t+1}+\gamma v_{\pi}(S_{t+1})\mid S_{t}=s, A_{t}=\pi'(a)]
\\&=\mathbb{E}_{\pi'}[r_{t+1}+\gamma v_{\pi}(S_{t+1})\mid S_{t}=s]
\\& \leq \mathbb{E}_{\pi'}[r_{t+1}+\gamma q_{\pi}(S_{t+1},\pi'(S_{t+1}) \mid S_{t}=s]
\\& = \mathbb{E}_{\pi'}[r_{t+1}+\gamma \mathbb{E}_{\pi'}[r_{t+2}+\gamma v_{\pi}(S_{t+2})]\mid S_{t}=s]
\\& = \mathbb{E}_{\pi'}[r_{t+1}+\gamma r_{t+2}+\gamma^2 v_{\pi}(S_{t+2})\mid S_{t}=s]
\\& \leq \ldots
\\&= v_{\pi'}(s)
\end{split}
\end{equation}
\end{proof}


\begin{algorithm}
	\SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    
	\Input{arbitrary policy, stopping criterion $\epsilon$}
	\Output{estimation of the optimal policy and of its value function}
	
	Policy evaluation :\\
	    \While{$\Delta \geq \epsilon$}{
    	$\Delta = 0$\\
    	\For{$s\in \mathcal{S}$}{
    	 $v=V(s)$\\
    	 $V(s)=\underset{a}{\sum}\pi(a\mid s)\underset{s'}{\sum}\mathbb{P}(s'\mid a,s)[R(s')+\gamma V(s')]$\\
    	 $\Delta = \max(\Delta,\mid v-V(s)\mid)$\\
    	 }
       	}
     
     Policy improvement : \\
     policy-stable = true \\
     \For{$s \in \mathcal{S}$}{
     old-action= $\pi(s)$\\     
     $\pi(s)=\underset{a \in A}{\text{argmax}}q_{\pi}(s,a)$\\
     \If{old-action $\neq \pi(s)$}{
     policy-stable = false}
     }
     \eIf{policy-stable}{
     \Return{$V \approx v^{*} \text{and} \quad \pi \approx \pi^{*}$}
     }{Go to Policy evaluation}
     
	\caption{Policy Iteration}
	\label{algo3}
\end{algorithm}







\subsection{Other types of DP method}

\paragraph{Value Iteration}
Value iteration is another way of approximating an optimal policy : it combines in each of its sweep improvement and evaluation in algorithm \ref{algo4}.

\begin{algorithm}[H]
	\SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    \label{algo4}
    
    \Input{policy}
    \Output{estimate of optimal policy}    
	    
    Initialize V arbitrarily 
    \While{$\Delta \geq \epsilon$}{
    	$v=V(s)$\\
    	$V(s)=\underset{a}{\max}\underset{s'}{\sum}\mathbb{P}(s'\mid s,a)[R(s')+\gamma v_{k}(s')]$\\
    	$\Delta =\max(\Delta,\mid v-V(s) \mid)$\\    	
    }
    \Return{$\pi \approx \pi^{*}$ s.t : $\pi(s)=\underset{a}{\text{argmax}}\underset{s'}{\sum}\mathbb{P}(s'\mid s,a)[R(s')+\gamma v_{k}(s')]$}
    

    
\caption{Value iteration}
\end{algorithm}



Algorithm \ref{algo4} finishes if the number of states is finite : 
\begin{proof} 
We define the Bellman operator : $\mathcal{T}:\mathbb{R}^{\mid \mathcal{S}\mid } \mapsto \mathbb{R}^{\mid \mathcal{S}\mid }$ in the following way :
\begin{equation*}
(\mathcal{T}V)(s)=\max_{a \in \mathcal{A}}\sum_{s'}\mathbb{P}(s'\mid a,s)\left[ R(s') + \gamma V(s') \right]
\end{equation*}

In particular this operator is a contraction in the infinity norm, and hence our convergence is equivalent to the well known fixed point problem. Indeed if we consider the inequality : 
\begin{equation}
\mid \max_{z}f(z)-\max{z}h(z)\mid \leq \max_{z}\mid f(z) - h(z)\mid
\label{inter}
\end{equation}
Then having this inequality:
\begin{equation*}
\begin{split}
\mid \mathcal{T}V(s) - \mathcal{T}V'(s) \mid & \leq \mid \max_{a \in \mathcal{A}}\sum_{s'}\mathbb{P}(s'\mid a,s)\left[ R(s') + \gamma V(s') \right] - \max_{a \in \mathcal{A}}\sum_{s'}\mathbb{P}(s'\mid a,s)\left[ R(s') + \gamma V'(s') \right]\mid
\\& \stackrel{(\ref{inter})}{\leq} \max_{a \in \mathcal{A}}\mid \sum_{s'}\mathbb{P}(s'\mid a,s)\left[ R(s') + \gamma V(s') \right] - \sum_{s'}\mathbb{P}(s'\mid a,s)\left[ R(s') + \gamma V'(s') \right]\mid
\\&\leq \max _{a}\gamma \mid \sum_{s'}\mathbb{P}(s'\mid a,s)\left[ V(s')-V'(s')\right] \mid
\\& \leq \gamma \max_{s} \mid V(s)-V'(s) \mid
\\& \leq \gamma \mid \mid V-V'\mid \mid _{\infty}
\end{split}
\end{equation*}

Now using the property that a contraction has a unique fixed point and that all sequences $V,\mathcal{T}V,\mathcal{T}^2V,\ldots $ converges towards this fixed point we get the convergence.


\end{proof}





There exists some others dynamic programming algorithm to solve these problems : 
\begin{itemize}
\item Asynchronous Dynamic programming : it doe not sweep amongst all the states at each iteration. They are in place iterative DP algorithms.
\item General Policy Iteration (GPI): they are mixing the two components of policy iteration (evaluation and improvement) a little bit more than the algorithm we already saw.
\end{itemize}

But the most important drawback to these methods is that they are model-based, meaning that we need to have a complete knowledge of the model to implement those.

\section{model-free approaches and algorithms}

Until now, we assumed implicitly that we knew the model (i.e. the transition function) of our environment all the previous learning algorithm where based on this assumption. Hence we call them \emph{model based}, meaning that you can not apply them if you do not know the model. But in real life applications, we often do not know the whole model, and we have to study other methods in order to do reinforcement learning : this is what gives us the \emph{model free} algorithms. We will discuss shortly some of them in this section.

\subsection{Monte-Carlo methods}
Monte-Carlo methods use sampling from episodes to estimate the action value function.
They differ from the dynamic programming methods by the fact that they do not require a knowledge of the model to estimate the action value function of a policy. To improve our policy, we still use the same greedy approach as before, choosing the best actions available in each state.
Monte-Carlo methods estimate the value function by averaging the sample return from the episodes. Thus they only need sample sequences of state, action, reward from experience or simulation. \citep{Sutton}


 It can be shown to converge as long as all pairs $(s,a)$ with $s$ a state, and $a$ an action are visited an infinite number of time.
\subsection{Temporal difference methods}
Temporal difference methods are a combination of dynamic programming and Monte-Carlo methods : unlike Monte Carlo methods, TD learning methods do not have to wait until an estimate of the return is available (i.e., at the end of an episode) to update the value function. Instead, they use temporal errors and only have to wait until the next time step. The temporal error is the difference between the old estimate and a new estimate of
the value function, taking into account the reward received in the current sample.
These updates are done iteratively and, in contrast to dynamic programming
methods, only take into account the sampled successor states. \citep{Xia2015}

\paragraph{Q-learning} Estimation of the optimal action value function $q_*$ given in Equation (\ref{bellman_opt_q}):


with $s'$ the state resulting from action a :
\begin{equation}
Q(s,a) \leftarrow Q(s,a) + \alpha \left( \mathcal{R}(s') + \gamma \max_{a'}Q(s',a') - Q(s,a) \right) 
\label{eq:ql}
\end{equation}

Where $\alpha$ is the learning factor. The actions are chosen so that we balance the exploration exploitation issue (random, greedy in the limit (GLIE),\ldots). Episode in algorithm \ref{Ql:algo} is a list of episodes, one episode $\mathcal{E}$ being a finite successions of transitions  $<s,a,r,s'>$

\begin{algorithm}
	\SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    
	\Input{Q initialized at $0$, learning rate $\alpha$, discount factor $gamma$, exploration parameter $\epsilon$, Episodes}
	\Output{estimation of the optimal value function $Q$ and optimal policy $\pi$}
	
	Approximation of the value function
	    \For{$\mathcal{E}$ in Episodes}{
    		\For{ $<s,a,r,s'>\quad \in \mathcal{E}$}{
    	 $Q(s,a) \leftarrow Q(s,a) + \alpha \left( \mathcal{R}(s') + \gamma \max_{a'}Q(s',a') - Q(s,a) \right)$
    	 }
       	}
     
     Policy  : \\
     \For{$s \in \mathcal{S}$}{
     	$\pi(s)=\underset{a \in A}{\text{argmax}} ~Q(s,a)$\\
     }
     
	\caption{Q-learning algorithm}
	\label{Ql:algo}
\end{algorithm}

\begin{proof} (convergence of Q-learning algorithm)

As long as each pairs $(s,a)$ are visited infinitely many often, that $s'\sim T(s,a,s')$, $r \sim \mathcal{R}(s')$ and  $\underset{n \in \mathbb{N}}{\sum} \alpha = \infty$, $\underset{n \in \mathbb{N}}{\sum} \alpha^2 < \infty$, the Q-learning algorithm \ref{Ql:algo} converges.

See \citep{Watkins_article}

\end{proof}


\paragraph{SARSA} Estimation of the action value function $q_{\pi}$:

with the action $a'$ being chosen following the policy $\pi$:
\begin{equation}
Q(s,a) \leftarrow Q(s,a) + \alpha \left( \mathcal{R}(s') + \gamma Q(s',a') - Q(s,a) \right)
\end{equation}


In this project the purpose is to implement Q-learning.



\newpage
\part*{Implementation}
\fancyhead[L]{\emph{IMPLEMENTATION}}
We have to determine our states, and hence a way for the robot to find its relative distance to our traffic light.
\section{The robot}
The robot is a raspberry pi 3 powered robot with a camera, like in Figure (\ref{fig:sub1}), and it follows a black line on the floor. On the right of this line, we put our \emph{traffic light}, which is as one can see in Figure \ref{vue_robot}: the position of the activated region of the traffic light indicates the color: down is green, \ldots

\begin{figure}[H]
\centering
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.5\linewidth]{img/robot.png}
  \caption{The raspberry pi 3 powered robot with one camera}
  \label{fig:sub1}
\end{subfigure}%
\begin{subfigure}{.5\textwidth}
  \centering
  \includegraphics[width=.5\linewidth]{img/setting1_mod1.png}
  \caption{The setting}
  \label{fig:sub2}
\end{subfigure}
\caption{The Framework of our runs}
\label{fig:test}
\end{figure}



\section{The task}
The robot can already follow lines from previous projects on it, and we want to learn how to respect a traffic light using reinforcement learning. In this project the robot is only allowed to go forward or to stop, but cannot go backwards.

\section{Modelization}

\subsection{States}
The states are modeled in the following way : in one state we have to have the information about the distance to the traffic light if there is one, the color of the traffic light and the actual speed of the robot, along with the action that brought our agent into this state and the time spent in the color of the traffic light. Indeed, as the first informations (distance, speed, color of the traffic light) seem intuitive, we need the time spent in the color of the traffic light since it helps predict the behavior of the traffic light and then adapt our behavior to the traffic light : if we are driving a car and the traffic light is orange, depending for how long it has been in this state, we may speed up in order to pass before it turns to red. So we wanted to allow for this possible adaptation from the learner; for the action that brought us in the state, this is done in order to control the smoothness of the driving : in the reward function (see \ref{reward function}), we will be able to impose negative reward if the driving is too chaotic.

One can represent the states in the following way : we have a graph like the one in Figure  (\ref{figure : graph_example}) for each speed, for example in Figure~(\ref{figure : graph_ex}). Then we add as a second layer the information about the traffic light color, then a next one for the time spent in the color, and finally one for the action that brought the agent in this position.
\begin{figure}[H]
\begin{center}
    \begin{tikzpicture}[font=\sffamily]
    % Add the states
        \node[state,text=black] (d0) {d0};    
        \node[state,right = 1cm of d0,text=black] (d1) {d1};
        \node[state,right = 1cm of d1,text=black] (d2) {d2};
        \node[state,right = 1cm of d2,text=black,draw=none] (d3) {};       
              
        % Connect the states with arrows
        \draw[every loop,auto=right,>=latex,draw=black,fill=black]
        	(d0) edge[] (d1)
        	(d1) edge[] (d2)
        	(d2) edge[dashed] (d3)
            ;
    \end{tikzpicture}
\end{center}
    \caption{Exemple of a transition graph for a certain speed}
    \label{figure : graph_ex}

\end{figure}


The robot can go from speed $20$ to speed $100$  and we decided to consider the speeds $0,20,30,30,40,\ldots, 70$ for simplicity and we stopped at $70$ since if the robot goes faster, it has trouble following the line, and the distance detection becomes not reliable as Figure \ref{fig:mean and std of net44} shows.


The distance to the traffic light is computed in the following way. The traffic light is actually a phone, which accesses the website \url{http://light.grathink.com}. The robot perceives images as the image~(\ref{vue_robot}) and the algorithm for the image processing is algo (\ref{detection_image}). The distance is computed as the distance of the reference point in pixels.

\begin{figure}[h]
        \centering
        \captionsetup{justification=centering,margin=2cm}

        \begin{subfigure}[b]{0.49\textwidth}
            \centering
            \includegraphics[width=\textwidth]{img/vue7.png}   
            \label{fig:mean and std of net14}
        \end{subfigure}
        \begin{subfigure}[b]{0.49\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{img/vue10.png}
            \label{fig:mean and std of net24}
        \end{subfigure}
        \vskip\baselineskip
        
        \begin{subfigure}[b]{0.49\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{img/vue46.png}
   
            \label{fig:mean and std of net34}
        \end{subfigure}
        \begin{subfigure}[b]{0.49\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{img/vue51.png}
  
            \label{fig:mean and std of net44}
        \end{subfigure}
        \caption{{\small View from the robot's camera in our setting}}
	\label{vue_robot}
\end{figure}







\begin{figure}[h]
        \centering
        \begin{subfigure}[b]{0.475\textwidth}
            \centering
            \includegraphics[width=\textwidth]{img/dist30.png}
            \caption[Speed 30]%
            {{\small Speed 30}}    
            \label{fig:mean and std of net14}
        \end{subfigure}
        \hfill
        \begin{subfigure}[b]{0.475\textwidth}  
            \centering 
            \includegraphics[width=\textwidth]{img/dist50.png}
            \caption[]%
            {{\small Speed 50}}
            \label{fig:mean and std of net24}
        \end{subfigure}
        \vskip\baselineskip
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{img/dist70.png}
            \caption[]%
            {{\small Speed 70}}    
            \label{fig:mean and std of net34}
        \end{subfigure}
        \quad
        \begin{subfigure}[b]{0.475\textwidth}   
            \centering 
            \includegraphics[width=\textwidth]{img/dist100.png}
            \caption[]%
            {{\small Speed 100}}    
            \label{fig:mean and std of net44}
        \end{subfigure}
        \caption{{\small Evolution of the perceived distance at different speed}}
	\label{distance_plots}
\end{figure}

\begin{algorithm}
	\SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}
    
	\Input{an image from the camera feed of the robot}
	\Output{distance, color of the traffic light}
	
	Scan the searching area for bright white pixels\\
	Find the middle of the zone of the white pixels\\
    From there go down to the bottom of the traffic light (end is detected as a value in all the RGB channel big enough)\\
    distance = the number of pixels from the top of the image to the bottom of the traffic light\\
    color is determined from the intensity of the different RGB channel
	\caption{Image processing}
	\label{detection_image}
\end{algorithm}

Now our robot has a way to perceives in which state he is. What actions can he take ? 


\subsection{Actions}
The action we are interested in do not concern the direction, which is imposed from the line on the ground as we can see from Figure (\ref{fig:sub2}). Hence we are only interested in accelerating or slowing down. The possible actions are to go from one speed to the closest speed (can be easily done through the already coded functions on the robot), i.e. going from $40$ to $50$ or to $30$. This is in order to simulate smooth driving.

\subsection{Reward function} \label{reward function}

The reward function is defined so that the robot would go forward toward the traffic light, but adapts its speed with respect to the traffic light. In order to do that, we define all the states in which the position is after the traffic light as absorbing state: we do not consider how to behave once we have passed the traffic light or what to do when there is none, this can be easily coded and is not the purpose of this project.

The reward function used here is the one given by $\mathcal{R}(s,a)$, which means it takes into account the action we just did.

In order to define the reward function, we have to take a look at the behavior we want to make the robot learn: 
\begin{itemize}
\item If the robot plows through a red light, it should get a negative reward, but if it does it should get a positive one.
\item If the robot stops when the traffic light is not red, it should get a negative reward and a positive for the opposite action.
\item If the actions following one another are inconsistent with each other (hits the break then goes full speed for example), we will give a negative reward whatever the position, as long as it is not an absorbing position. This is why we use the form of the reward function given before.
\end{itemize}

The reward function is actually really sensitive: if the relative importance of the rewards given for a certain transitions is slightly changed, this can completely modify the behavior of the agent.




The precise parameters of the reward function where given by the results from experience, the one we used can be seen in the appendix \ref{code}.

\section{Simulation of the model}

One can apply Q-learning on a modelization of the problem : we do not have to have the robot running around into walls and whatever to learn; if we can have a simulation of our problem, we could run everything in a simulated world and after that actually transfer the results to the robot directly.

In order to do that, we had to actually simulate the environment of the robot, i.e. the transition function in the definition of Markov decision processes (see Section (\ref{def_MDP})).


We decomposed the simulation in multiple parts: 
\begin{itemize}
\item at a given speed we needed to know the evolution of our newly defined distance. This gave data as can be seen in Figure \ref{distance_plots}, the framework is described in Appendix \ref{pictures}. But even with a lot of data sampling, we were not able to completely define the transitions, some portions were still missing. In order to solve this issue, we used an imputation process like \emph{hot deck}: when a datum was missing, it was imputed using information from the nearest data (if in a certain position there were no data, we used the information given by positions close to this one, computed a median of the data from these distances and then imputed it with a little noise).

 We then computed probability of transition from one position to another while being at a certain speed.
\item we need to know the transitions of the color of the traffic light. In order to do that we measured what was the speed of the image processing, in order to know how many images the robot processes in average in one second. From these data, we coded a traffic light that simulated a real one with the following time (in seconds): green $5$, orange $2$, red $3$, orange $2$, \ldots
\item the transitions between speeds are supposed deterministic, we do not account for any problems in the connection of the pins of the robot. But one can note that the transitions are in such way that if you decide to slow down while already in the lowest speed, you just stay in low speed, and the same phenomena happens while accelerating in the highest possible speed.
\end{itemize}

Then, having a simulation of our model, we are able to make our robot learn on it.



\section{Q-learning implementation}


We apply the Q-learning algorithm described in Equation (\ref{eq:ql}), with parameters $\alpha = 0.1$ and $\gamma = 0.9$. The $\epsilon$ parameter for balancing exploration and exploitation is decreasing from $0.1$ to $0$ linearly as we learn (decayed $\epsilon$ or greedy in the limit (GLIE)). We learn over $5$ millions of episodes, one episode consisting on starting in front of a traffic light and stopping when we went passed a certain stopping distance from the traffic light (distance is linked to the fact that passed a certain point the camera of the robot does not see the light anymore).


\begin{figure}[h]
\centering
\captionsetup{justification=centering,margin=2cm}
\includegraphics[scale=0.6]{img/Q_learning_curve.png}
\caption{learning curve of the Q-learning algorithm \ref{Ql:algo} applied to our problem, $1$ million iterations}
\label{curve}
\end{figure}

We can verify how well the robot learns by visualizing the trajectory it would do: in Figure (\ref{trajectories}), we have the distance from the traffic light in the x axis , and on the y axis we have the speed. On the bottom of the graph we have the color the traffic light has during the run (the yellow color is representing the orange that comes after the red in the traffic light sequence): if there is a "pile" of points, it means the traffic light changed multiple times during the moment the robot was on this position. The robot is set to have a beginning distance of $200$ for data management purpose, so we can identify the beginning of a new episode from an error in the distance computation; that is why at the beginning of the trajectories there is this strange behavior of going from $200$ to about $125$ in a single shot.

From Figure \ref{trajectories} and Figure \ref{failure} in Annexe \ref{GRAPHS}, the simulation gives a non zero probability of staying in the same distance to the traffic light in non negative speed: this is due to the fact that there are imprecisions in the way we compute the distance to the traffic light. Then we can see that in Figure \ref{trajectories}, on the top right the robot actually stops, wait for the traffic light to turn orange and then speed up, exactly the behavior we wanted it to have. On the other hand, as one can see in Figure \ref{failure}, there is some cases where the robot fails and goes right pass a red light.

\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{img/traj1.png}
\caption{Graphical representation of  policies obtained from Q-learning with $100$ thousands  iterations}
\label{trajectories}
\end{figure}



\section{Conclusion}

We started this project with a robot that can already follow lines, and we used the technique implemented using support vector machine. The purpose of this bachelor project was to use Markov decision processes in the reinforcement learning framework, so in order to do that we choose to tackle the task to adapt the speed of the robot with respect to the traffic light.

We first had to implement an efficient algorithm to detect the traffic light in order to measure a distance we had to define from the traffic light and in order to see what color was the signalization in. We did so using \texttt{OpenCV}, this was the first big challenge we encountered.

Then we had to create a simulation of the environment, in order to be able to actually apply reinforcement learning in the form of Q-learning. In order to do so we had to run multiple times the robot to get data directly from it, manage the data we collected and then extrapolate from them.

Then while doing the learning we had to try and improve the reward function, which is the backbone of reinforcement learning: trough trial and error we finally found one acceptable, but there is room for more improvement, by doing a systematic comparisons of the policy obtained from different reward function and improving it that way for example.\\



What could be interesting to do next is to add another raspberry pie to the robot and use two cameras: a perception of depth would give much more accurate distance from the traffic light, and having more computational power would mean being able to do deep-learning to detect in a more general way traffic light, so we will not have to restrict ourselves to the traffic light we used.



\newpage
\part*{Appendix}
\fancyhead[L]{\emph{APPENDIX}}
\appendix
\section{Pictures} \label{pictures}

\begin{figure}[h]
\centering
\captionsetup{justification=centering,margin=2cm}
\includegraphics[scale=0.6]{img/Q_learning_curve_random_light.png}
\caption{learning curve of the Q-learning algorithm \ref{Ql:algo} applied to our problem, $100$ thousands iterations, but with changing light every $30$ thousands iterations: we can see the drop in the performance at iteration $90$ thousands}
\label{curve_random}
\end{figure}


\section{Graphs} \label{GRAPHS}


\begin{figure}[H]
\centering
\includegraphics[scale=0.4]{img/traj3.png}
\caption{Graphical representation of  policies obtained from Q-learning with $100$ thousands  iterations}
\label{failure}
\end{figure}


\section{Code} \label{code}

All the code can be found in the following git:

 \url{https://github.com/dufourc1/reinforcement-learning-and-autonomous-robot-navigation.git}

%\lstlistoflistings
%\vspace{1cm}


%\begin{lstlisting}[language=Python, caption=Reward Function used in this project]
%def reward(self,action):
%        stopping_distance = self.stop
%        r = 0
%        if self.position > stopping_distance:
%            r = -1
%            if self.last_action == 0 and action == 2 :
%                r = - 30
%            elif self.last_action == 2 and action == 0:
%                r = - 30
%        if self.position <= stopping_distance :
%            self.ended = True
%            if self.current_color == "red":
%                if self.speed == 20 :
%                    r = + 120
%                else :
%                    r = - 200
%            elif self.current_color == "green":
%                if self.speed == 20 :
%                    r = - 120
%                else : 
%                        r = + 120
%            else :
%                if self.speed != 20:
%                    r = + 120
%                else :
%                    r = - 120
%
%        self.cumulative_reward += r
%        return r
%\end{lstlisting}










\nocite{Watkins_phd}
\newpage
\fancyhead[L]{\emph{BIBLIOGRAPHY}}


\bibliographystyle{apa-good}
\bibliography{ref}
\addcontentsline{toc}{section}{References}


\end{document}
