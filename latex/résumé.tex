\documentclass[14pt,a4paper]{article}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{makeidx}
\usepackage{amsthm}
\usepackage[margin=48pt]{geometry}

\def\code#1{\texttt{#1}}
\def\iff{\Leftrightarrow}
\theoremstyle{definition}
\newtheorem{mydef}{Definition}


\oddsidemargin  0in
\evensidemargin  0in
\textwidth   6.3in
\textheight  9.5in
\topmargin  -0.7in



\usepackage{fancyhdr}
\pagestyle{fancy}
\lfoot{Charles DUFOUR}
\rfoot{\today}



\author{Charles Dufour}
\title{Bachelor Project : \\
Reinforcement learning and robot navigation}
\begin{document}

\begin{titlepage}
\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page
 
%----------------------------------------------------------------------------------------
%   HEADING SECTIONS
%----------------------------------------------------------------------------------------

\vspace{3cm}
\textsc{\LARGE \'Ecole polytechnique f\'ed\'erale de Lausanne}\\[0.5cm] % Name of your university/college
\textsc{\large Disopt}\\[1.5cm] % Name of your university/college
\textsc{\LARGE Semester project}\\[0.5cm] % Major heading such as course name
\textsc{\large }\\[0.5cm] % Minor heading such as course title

%----------------------------------------------------------------------------------------
%   TITLE SECTION
%----------------------------------------------------------------------------------------

\HRule \\[0.4cm]
{ \huge \bfseries Reinforcement learning and robot navigation}\\[0.4cm] % Title of your document
\HRule \\[1.5cm]
 
%----------------------------------------------------------------------------------------
%   AUTHOR SECTION
%----------------------------------------------------------------------------------------

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Student:}\\
Charles Dufour
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Supervisors:} \\
Jonas Racine\\% Supervisor's Name
Prof. Friedrich Eisenbrand 
\end{flushright}
\end{minipage}\\[5cm]

%----------------------------------------------------------------------------------------
%   LOGO SECTION
%----------------------------------------------------------------------------------------

% \includegraphics[width=0.5\linewidth]{Logo}\\[1cm] % Include a department/university logo - this will require the graphicx package
 
%----------------------------------------------------------------------------------------

\vfill % Fill the rest of the page with whitespace

\end{titlepage}



%\begin{titlepage}
%   \vspace*{\stretch{1.0}}
%   \begin{center}
%      \Large\textbf{Reinforcement learning and robot navigation}\\
%      
%      \large\textbf{ }\\
%      \large\textit{Bachelor Project}
%   \end{center}
%   \vspace*{\stretch{2.0}}
%   
%   \vfill 
%   {\centering Charles Dufour, EPFL\par}
%\end{titlepage}

\newpage

\tableofcontents

\newpage
\section{Theory}
\subsection{Introduction}


\emph{Reinforcement learning}\\
Reinforcement learning is learning what to do,how to map situations to actions so as to maximize
a numerical reward signal. The learner is not told which actions to take, but instead must discover
which actions yield the most reward by trying them. \cite{Sutton}

Some technical terms : 
\begin{itemize}
\item \emph{policy} : it is a mapping from the states to the actions
\item \emph{value function} : what our learning agent is trying to optimize 
\item \emph{model} of the environment : the laws governing the environment 
\end{itemize}

"Reinforcement learning methods specify how the agent's policy is changed as a result of its experience"\cite{Sutton}



The usual way to formulate the reinforcement learning problem from a mathematical point of view is by using what we call Markov's decision processes.

\subsection{MDP : Markov decision processes}



Markov's decision processes are composed by : 

\begin{itemize}
\item a set of states : $\mathcal{S}$
\item a set of actions  : $A$
\item a transition function :  $T(s,a,s') \sim  Pr(s'\mid a, s) \quad s,s' \in \mathcal{S}$ which gives the state transition probabilities
\item a reward function : $\mathcal{R}:\mathcal{S}\mapsto \mathbb{R} $
\item The Markov property : the transitions only depends on the current state and action
\end{itemize}



\paragraph{The Bellman equation}
\begin{equation}
v_{\pi}(s)=R(s)+\gamma \sum_{s \in \mathcal{S}} P(s' \mid s,\pi(s))v_{\pi}(s')
\label{bellman1}
\end{equation}

\begin{equation}
v_{\pi}(s)=\sum_{a}\pi(a\mid s)\sum_{s',r}p(s',r \mid s,a)[r+\gamma v_{\pi}(s')]
\label{bellman2}
\end{equation}

These are two formulations of the bellman equation used to compute optimal policies by iteration : \eqref{bellman1} is from the MOOC and \eqref{bellman2} is from Sutton's book \cite{Sutton}

\subsubsection{Policies and Value functions}

A policy : $\pi : \mathcal{S} \mapsto A $ is a mapping from states to action. If we follow this policy (way of behaving) we can define the value function for a policy in order to compare them, which links a state and its expected reward if we follow this policy : 

The discount factor helps making our learning agent more or less far-sighted : the greater $\gamma $ the more "impact" will have a late reward on our reward sequence, hence making the agent more conscious about these actions.

We define the return as : $G_t= R_{t+1}+\gamma R_{t+2}+ \dots = \sum_{k=0}^{\infty}\gamma^{k}R_{t+k+1} $

And $\pi(a\mid s) $ is the probability that $A_t=a$ if $S_t=s$

Then we can define the value of taking action $a$ in state $s$ while following the policy $\pi$ 

\begin{equation}
q_{\pi}(s,a)= \mathbb{E}[G_t\mid S_t=s, A_t=a]=\mathbb{E}[\sum_{k=0}^{\infty}\gamma^{k}R_{t+k+1}\mid S_t=s,A_t=a]
\end{equation}


And the value of a state s under a policy:
\begin{equation}
v_{\pi}(s)= \mathbb{E}[G_t \mid S_t=s]= \mathbb{E}[ \sum_{k=0}^{\infty}\gamma^t R_{t+k+1} \mid S_t = s ]\quad \forall s \in \mathcal{S}
\end{equation}

\subsubsection{Optimal policies and Optimal value function}

For finite MDP's, the value function can define a partial order in the space of policies : 
$$ 
\pi\leq \pi' \iff \pi (s)\leq \pi' (s) \quad \forall s \in \mathcal{S}
$$

An optimal policy is a policy which is greater or equal than any other policy.

\paragraph{Bellman optimality equations}

\begin{equation}
v_{*}(s)=\max_{a} \sum_{s',r}p(s',r\mid s,a)[r+\gamma v_{*}(s')]
\label{bellman_opt_v}
\end{equation}

\begin{equation}
q_{*}(s,a)= \sum_{s',r}p(s',r \mid s,a)[r+\gamma \max_{a'}q_{*}(s',a')]
\label{bellman_opt_q}
\end{equation}



For finite MDP's these equations have a unique solution. We can note that if we know $v_{*}$ or $q_{*}$ a greedy approach to define a policy (best in the short term) becomes a long-term optimal solution.


\subsection{Solving MDP's}

In general we don't have all the information we need to compute the exact value of $v_{*}$ or even if we have them, we don't have the computational 
power needed. We often use approximation of value-function instead.

\subsubsection{Dynamic programming}







\newpage

\section{Journal}
\begin{description}
\item[28.02.2018] finished reading chapter 2 Sutton's book about the \code{k-bandits problem} : implementation of simple algorithms of the book on jupyter notebook in \code{Rl-sandbox}
\item[01.03.2018] initiated the Latex journal 
\item[04.03.2108]read the notebook about \code{numpy} and tried to go on with the lecture of the literature--> have to read again the example about the golf 

finished the chapter 3 
\item[12.03.2018] read chapter three again and made a summary of it

Then tried to attack the street racer problem 
\end{description}


\newpage

\bibliographystyle{plain}
\bibliography{ref}
\addcontentsline{toc}{section}{References}


\end{document}
